<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://alphapibeta.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://alphapibeta.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-20T07:32:55+00:00</updated><id>https://alphapibeta.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">GPU Optimization for Convolution Operations</title><link href="https://alphapibeta.github.io/blog/2024/GPU-Optimization-for-Convolution-Operations/" rel="alternate" type="text/html" title="GPU Optimization for Convolution Operations"/><published>2024-09-20T00:00:00+00:00</published><updated>2024-09-20T00:00:00+00:00</updated><id>https://alphapibeta.github.io/blog/2024/GPU-Optimization-for-Convolution-Operations</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2024/GPU-Optimization-for-Convolution-Operations/"><![CDATA[<h1 id="1-introduction">1. Introduction</h1> <p>In this blog we are gonna explore the convolution operation, and how accleration of this operation uisng the Nvidia-cuda toolkit and various perfoamce measure and metrics along the way.</p> <h1 id="2-theoretical-foundations-of-convolution-operations">2. Theoretical Foundations of Convolution Operations</h1> <h2 id="21-mathematical-definition">2.1 Mathematical Definition</h2> <p>For an input tensor $\mathcal{I}$ and a filter tensor $\mathcal{F}$, the 2D convolution operation can be expressed as:</p> \[\mathcal{O}_{n,k,i,j} = \sum_{c=0}^{C_{in}-1} \sum_{p=0}^{K_h-1} \sum_{q=0}^{K_w-1} \mathcal{I}_{n,c,i+p,j+q} \cdot \mathcal{F}_{k,c,p,q}\] <p>Where:</p> <ul> <li>$\mathcal{O}$ is the output tensor</li> <li>$n$ is the batch index</li> <li>$k$ is the output channel index</li> <li>$i, j$ are spatial indices in the output</li> <li>$c$ is the input channel index</li> <li>$p, q$ are spatial indices in the filter</li> <li>$C_{in}$ is the number of input channels</li> <li>$K_h, K_w$ are the height and width of the filter</li> </ul> <h2 id="22-tensor-dimensions">2.2 Tensor Dimensions</h2> <p>Given:</p> <ul> <li>Input tensor $\mathcal{I} \in \mathbb{R}^{N imes C_{in} imes H_{in} imes W_{in}}$</li> <li>Filter tensor $\mathcal{F} \in \mathbb{R}^{C_{out} imes C_{in} imes K_h imes K_w}$</li> </ul> <p>The output tensor $\mathcal{O}$ will have dimensions:</p> \[\mathcal{O} \in \mathbb{R}^{N imes C_{out} imes H_{out} imes W_{out}}\] <p>Where: \(H_{out} = H_{in} - K_h + 1\) \(W_{out} = W_{in} - K_w + 1\)</p> <h1 id="23-visual-representation-of-convolution-operation">2.3 Visual Representation of Convolution Operation</h1> <h2 id="example-configuration">Example Configuration</h2> <ul> <li><strong>Input Tensor</strong>: <code class="language-plaintext highlighter-rouge">N=1, C=4, H=4, W=4</code></li> <li><strong>Filter Tensor</strong>: <code class="language-plaintext highlighter-rouge">N=1, C=4, H=2, W=2</code></li> <li><strong>Output Tensor</strong>: The output tensor dimensions are calculated as follows:</li> </ul> <h3 id="output-dimensions-calculation">Output Dimensions Calculation</h3> <p>The output dimensions are computed as:</p> <p>\(H_{out} = H_{in} - K_h + 1 = 4 - 2 + 1 = 3\) \(W_{out} = W_{in} - K_w + 1 = 4 - 2 + 1 = 3\)</p> <p>Therefore, the output tensor has dimensions <code class="language-plaintext highlighter-rouge">N=1, C=1, H=3, W=3</code>.</p> <h2 id="231-visualizing-the-3d-convolution-operation">2.3.1 Visualizing the 3D Convolution Operation</h2> <h3 id="input-tensor-h4-w4-c4">Input Tensor (H=4, W=4, C=4)</h3> <p>Each layer represents one of the 4 channels of the input tensor.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Channel 1                Channel 2                Channel 3                Channel 4
-------------------      -------------------      -------------------      -------------------
| 1 | 2 | 0 | 1 |        | 1 | 0 | 1 | 2 |        | 0 | 1 | 2 | 1 |        | 1 | 2 | 1 | 0 |
| 0 | 1 | 3 | 2 |        | 2 | 1 | 0 | 1 |        | 1 | 2 | 0 | 1 |        | 2 | 0 | 1 | 1 |
| 1 | 2 | 1 | 0 |        | 0 | 1 | 2 | 0 |        | 1 | 1 | 0 | 2 |        | 0 | 1 | 2 | 2 |
| 2 | 1 | 0 | 1 |        | 2 | 0 | 1 | 1 |        | 2 | 0 | 1 | 0 |        | 1 | 0 | 1 | 1 |
-------------------      -------------------      -------------------      -------------------
</code></pre></div></div> <h3 id="filter-tensor-h2-w2-c4">Filter Tensor (H=2, W=2, C=4)</h3> <p>Each layer represents one of the 4 channels of the filter tensor.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Channel 1                Channel 2                Channel 3                Channel 4
-------------------      -------------------      -------------------      -------------------
| 1 | 0 |                | 0 | 1 |                | 1 | 0 |                | 0 | 1 |
| 0 | 1 |                | 1 | 0 |                | 0 | 1 |                | 1 | 0 |
-------------------      -------------------      -------------------      -------------------
</code></pre></div></div> <h3 id="convolution-operation-visualization-3d">Convolution Operation Visualization (3D)</h3> <p>The convolution operation involves performing a dot product of the 4-channel filter tensor with the 4-channel input tensor over a 2x2 region.</p> <h4 id="3d-dot-product-representation">3D Dot Product Representation</h4> <p>We loop over <code class="language-plaintext highlighter-rouge">x---direction</code> and one in <code class="language-plaintext highlighter-rouge">y---direction input</code> : <code class="language-plaintext highlighter-rouge">[1,:,0:2,0:2]</code> and slice the array get <code class="language-plaintext highlighter-rouge">input: [1,:,:2,:2]* filter [1,:,:2,:2]</code> This colapses channles and just returns the output</p> <p>\(H_{out} = H_{in} - K_h + 1 = 4 - 2 + 1 = 3\) \(W_{out} = W_{in} - K_w + 1 = 4 - 2 + 1 = 3\)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3D Dot Product Operation for Each Spatial Position
+----------------------------+     +----------------------------+
| Input (4x2x2) Sub-Tensor   |     | Filter (4x2x2) Sub-Tensor  |
| for spatial position (0,0) |     |                          |
+----------------------------+     +----------------------------+
| [                         ] |     | [                         ] |
| Channel 1:                |     | Channel 1:                |
| [ 1 | 2 ]                 |     | [ 1 | 0 ]                 |
| [ 0 | 1 ]                 |     | [ 0 | 1 ]                 |
|                           |     |                           |
| Channel 2:                |     | Channel 2:                |
| [ 1 | 0 ]                 |     | [ 0 | 1 ]                 |
| [ 2 | 1 ]                 |     | [ 1 | 0 ]                 |
|                           |     |                           |
| Channel 3:                |     | Channel 3:                |
| [ 0 | 1 ]                 |     | [ 1 | 0 ]                 |
| [ 1 | 2 ]                 |     | [ 0 | 1 ]                 |
|                           |     |                           |
| Channel 4:                |     | Channel 4:                |
| [ 1 | 2 ]                 |     | [ 0 | 1 ]                 |
| [ 2 | 0 ]                 |     | [ 1 | 0 ]                 |
+----------------------------+     +----------------------------+
</code></pre></div></div> <h4 id="3d-convolution-result-at-spatial-position-00">3D Convolution Result at Spatial Position (0,0)</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Cumulative Sum for Output Tensor at (0,0):
1*1 + 2*0 + 0*1 + 1*0 + 1*0 + 0*1 + 2*1 + 1*0 + 0*1 + 1*0 + 1*1 + 2*0 + 1*0 + 2*1 + 0*0 + 2*0 = 8
</code></pre></div></div> <p>This value is stored in the output tensor at position (0,0). We repeat this process for all spatial positions.</p> <h3 id="final-output-tensor">Final Output Tensor</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output Tensor (H=3, W=3, C=1):
-----------------
|  8 |  7 |  6 |
|  5 |  8 |  9 |
|  7 |  5 |  4 |
-----------------
</code></pre></div></div> <p>Each value represents the cumulative dot product result for each corresponding spatial position in the input tensor.</p> <p>This visualization demonstrates how the filter interacts with the input tensor across all channels to generate the output tensor values.</p> <h1 id="24-padding-and-stride">2.4 Padding and Stride</h1> <h3 id="241-padding">2.4.1 Padding</h3> <p>Padding is used to control the spatial size of the output tensor. With padding, zeros are added to the input tensor around its border to allow the filter to slide outside the input’s original spatial dimensions.</p> <p><strong>Example with Padding:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Tensor (5x5 with padding of 1):
-------------------------
| 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| 0 | 1 | 2 | 0 | 1 | 2 | 0 |
| 0 | 0 | 1 | 3 | 2 | 1 | 0 |
| 0 | 1 | 2 | 1 | 0 | 1 | 0 |
| 0 | 2 | 1 | 0 | 1 | 0 | 0 |
| 0 | 1 | 2 | 0 | 1 | 2 | 0 |
| 0 | 0 | 0 | 0 | 0 | 0 | 0 |
-------------------------
</code></pre></div></div> <h3 id="242-stride">2.4.2 Stride</h3> <p>Stride controls how the filter convolves around the input tensor. If the stride is 1, the filter moves one pixel at a time. If the stride is 2, it moves two pixels at a time.</p> <p><strong>Example with Stride 2:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Tensor:
-----------------
| 1 | 2 | 0 | 1 |
| 0 | 1 | 3 | 2 |
| 1 | 2 | 1 | 0 |
| 2 | 1 | 0 | 1 |
-----------------
Stride 2:
-----------------
| 1 | 0 |
| 1 | 1 |
-----------------
Output Tensor (2x2):
-----------------
|  7 |  6 |
|  6 |  3 |
-----------------
</code></pre></div></div> <p>In this example, the filter skips every other position, resulting in a smaller output tensor.</p> <h3 id="243-combining-padding-and-stride">2.4.3 Combining Padding and Stride</h3> <p>When using both padding and stride, we can control the exact size of the output tensor.</p> <p><strong>Example:</strong></p> <ol> <li>Padding = 1, Stride = 2</li> <li>Input Tensor = 5x5 (with padding added)</li> <li>Output Tensor = 3x3</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Tensor with Padding:
-------------------------
| 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| 0 | 1 | 2 | 0 | 1 | 2 | 0 |
| 0 | 0 | 1 | 3 | 2 | 1 | 0 |
| 0 | 1 | 2 | 1 | 0 | 1 | 0 |
| 0 | 2 | 1 | 0 | 1 | 0 | 0 |
| 0 | 1 | 2 | 0 | 1 | 2 | 0 |
| 0 | 0 | 0 | 0 | 0 | 0 | 0 |
-------------------------

Output Tensor (3x3):
-----------------
|  4 |  5 |  2 |
|  3 |  6 |  1 |
|  5 |  3 |  3 |
-----------------
</code></pre></div></div> <p>This explains how padding and stride can be used to control the size and shape of the output tensor, allowing for more flexibility in convolution operations.</p> <h1 id="25-dilated-convolution">2.5 Dilated Convolution</h1> <h3 id="251-introduction-to-dilation">2.5.1 Introduction to Dilation</h3> <p>Dilated convolutions introduce “holes” in the filter, effectively increasing the receptive field without increasing the number of parameters or the amount of computation. This allows the model to capture more global information while maintaining the resolution.</p> <h3 id="252-dilated-convolution-operation">2.5.2 Dilated Convolution Operation</h3> <p>In a dilated convolution, the filter is applied over an input tensor with defined gaps, controlled by the dilation rate. For example, a dilation rate of 2 means skipping one element between every two filter elements.</p> <p><strong>Example with Dilation 2:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Tensor:
-----------------
| 1 | 2 | 0 | 1 |
| 0 | 1 | 3 | 2 |
| 1 | 2 | 1 | 0 |
| 2 | 1 | 0 | 1 |
-----------------
Dilation 2:
-----------------
| 1 | 0 |
| 1 | 1 |
-----------------
Output Tensor (2x2):
-----------------
|  4 |  2 |
|  3 |  4 |
-----------------
</code></pre></div></div> <h3 id="253-combining-dilation-with-padding-and-stride">2.5.3 Combining Dilation with Padding and Stride</h3> <p>Dilated convolutions can be combined with padding and stride to allow for more flexible receptive field adjustments.</p> <p><strong>Example:</strong></p> <ol> <li>Padding = 1, Stride = 1, Dilation = 2</li> <li>Input Tensor = 5x5</li> <li>Output Tensor = 3x3</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Tensor:
-----------------
| 1 | 2 | 3 | 4 | 5 |
| 6 | 7 | 8 | 9 | 0 |
| 1 | 2 | 3 | 4 | 5 |
| 6 | 7 | 8 | 9 | 0 |
| 1 | 2 | 3 | 4 | 5 |
-----------------
Filter:
-----------------
| 1 | 0 |
| 0 | 1 |
-----------------
Output Tensor (3x3):
-----------------
|  4 |  8 |  0 |
| 12 | 16 |  8 |
|  4 |  8 |  0 |
-----------------
</code></pre></div></div> <p>This demonstrates how dilation, padding, and stride can be used together to control the receptive field, tensor size, and level of detail captured in the convolution operation.</p> <h1 id="3-tensor-computations-on-gpus">3. Tensor Computations on GPUs</h1> <h2 id="31-gpu-memory-hierarchy">3.1 GPU Memory Hierarchy</h2> <p>GPUs have a complex memory hierarchy that affects the performance of tensor computations:</p> <ol> <li>Global Memory: Largest but slowest <ul> <li>Capacity: Typically several GB</li> <li>Latency: 400-800 clock cycles</li> </ul> </li> <li>Shared Memory: Fast, on-chip memory shared by threads in a block <ul> <li>Capacity: Typically 48KB - 96KB per SM</li> <li>Latency: ~20 clock cycles</li> </ul> </li> <li>Registers: Fastest, private to each thread <ul> <li>Capacity: Typically 256KB per SM</li> <li>Latency: ~1 clock cycle</li> </ul> </li> </ol> <h2 id="32-tensor-operations-in-cuda">3.2 Tensor Operations in CUDA</h2> <p>In CUDA, tensor operations are typically implemented using multi-dimensional thread blocks. For a 4D tensor operation, we might use a 3D grid of thread blocks:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">dim3</span> <span class="nf">threadsPerBlock</span><span class="p">(</span><span class="n">BLOCK_SIZE_X</span><span class="p">,</span> <span class="n">BLOCK_SIZE_Y</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="kt">dim3</span> <span class="nf">numBlocks</span><span class="p">(</span>
    <span class="p">(</span><span class="n">W_out</span> <span class="o">+</span> <span class="n">BLOCK_SIZE_X</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE_X</span><span class="p">,</span>
    <span class="p">(</span><span class="n">H_out</span> <span class="o">+</span> <span class="n">BLOCK_SIZE_Y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE_Y</span><span class="p">,</span>
    <span class="n">C_out</span>
<span class="p">);</span>
</code></pre></div></div> <p>Each thread then computes one or more elements of the output tensor:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">w</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">h</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>
</code></pre></div></div> <p>This mapping allows for efficient parallelization of tensor operations on GPUs.</p> <p>In the next section, we will delve into NVIDIA performance metrics and how they relate to optimizing convolution operations.</p> <h1 id="33-mapping-tensor-operations-to-cuda">3.3 Mapping Tensor Operations to CUDA</h1> <p>The given CUDA code performs a 2D convolution operation on an input tensor using the <code class="language-plaintext highlighter-rouge">NCHW</code> format, where:</p> <ul> <li><code class="language-plaintext highlighter-rouge">N</code> represents the batch size.</li> <li><code class="language-plaintext highlighter-rouge">C</code> represents the number of channels.</li> <li><code class="language-plaintext highlighter-rouge">H</code> and <code class="language-plaintext highlighter-rouge">W</code> represent the height and width of the tensor, respectively.</li> </ul> <h2 id="331-tensor-mapping-to-cuda-thread-blocks">3.3.1 Tensor Mapping to CUDA Thread Blocks</h2> <p>The input, filter, and output tensors are mapped to CUDA thread blocks and threads using 3D grid dimensions. Each thread computes a single element of the output tensor.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Tensor (NCHW)               Filter Tensor (OCHW)               Output Tensor (NCHW)
-----------------------           -----------------------            -----------------------
N = 1                             O = 2                              N = 1
C = 2                             C = 2                              C = 2
H = 5                             H = 3                              H = 3
W = 5                             W = 3                              W = 3

Input (1, 2, 5, 5)                Filter (2, 2, 3, 3)                Output (1, 2, 3, 3)
                                                                     
                                                                     
[ x x x x x ]                     [ w w w ]                          [ y y y ]   
[ x x x x x ]                     [ w w w ]                          [ y y y ]   
[ x x x x x ]                     [ w w w ]                          [ y y y ]   
[ x x x x x ]                                                       
[ x x x x x ]                                                       
                                                                     
[ x x x x x ]                     [ w w w ]                          [ y y y ]   
[ x x x x x ]                     [ w w w ]                          [ y y y ]   
[ x x x x x ]                     [ w w w ]                          [ y y y ]   
[ x x x x x ]                                                       
[ x x x x x ]                                                       
</code></pre></div></div> <h3 id="cuda-kernel-mapping">CUDA Kernel Mapping</h3> <ul> <li><strong>Grid Dimensions (<code class="language-plaintext highlighter-rouge">numBlocks</code>):</strong> Represents the number of blocks needed to cover the entire output tensor in 3D.</li> <li><strong>Block Dimensions (<code class="language-plaintext highlighter-rouge">threadsPerBlock</code>):</strong> Represents the number of threads in each block, matching the spatial dimensions of the output.</li> </ul> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">dim3</span> <span class="nf">threadsPerBlock</span><span class="p">(</span><span class="n">block_x</span><span class="p">,</span> <span class="n">block_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// 2D threads per block for spatial dimensions</span>
<span class="kt">dim3</span> <span class="nf">numBlocks</span><span class="p">((</span><span class="n">out_width</span> <span class="o">+</span> <span class="n">block_x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">block_x</span><span class="p">,</span> 
               <span class="p">(</span><span class="n">out_height</span> <span class="o">+</span> <span class="n">block_y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">block_y</span><span class="p">,</span> 
               <span class="n">out_channels</span><span class="p">);</span> <span class="c1">// 3D grid to cover all output elements</span>
</code></pre></div></div> <h2 id="332-cuda-thread-block-and-tensor-mapping">3.3.2 CUDA Thread Block and Tensor Mapping</h2> <p>Each CUDA thread block computes a subset of the output tensor, where each thread within a block calculates a single element of the output. Here is a visual representation of the mapping:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CUDA Thread Block Mapping
-------------------------

Output Tensor (1, 2, 3, 3)
                        0,0     0,1     0,2
                    +-----------------------+
              0,0   |(0,0)  |(0,1)  |(0,2)  |
                    |-------|-------|-------|
              0,1   |(1,0)  |(1,1)  |(1,2)  |
                    |-------|-------|-------|
              0,2   |(2,0)  |(2,1)  |(2,2)  |
                    +-----------------------+
                        0,0     0,1     0,2
</code></pre></div></div> <h2 id="333-convolution-computation-in-cuda">3.3.3 Convolution Computation in CUDA</h2> <p>The CUDA kernel loops over the batch size, input channels, and filter dimensions to compute the convolution as follows:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// CUDA Kernel for 2D convolution</span>
<span class="k">__global__</span>
<span class="kt">void</span> <span class="nf">convolution2DKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">filter</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> 
                         <span class="kt">int</span> <span class="n">batch</span><span class="p">,</span> <span class="kt">int</span> <span class="n">out_channels</span><span class="p">,</span> <span class="kt">int</span> <span class="n">in_channels</span><span class="p">,</span> 
                         <span class="kt">int</span> <span class="n">out_height</span><span class="p">,</span> <span class="kt">int</span> <span class="n">out_width</span><span class="p">,</span> 
                         <span class="kt">int</span> <span class="n">filter_height</span><span class="p">,</span> <span class="kt">int</span> <span class="n">filter_width</span><span class="p">,</span> 
                         <span class="kt">int</span> <span class="n">input_height</span><span class="p">,</span> <span class="kt">int</span> <span class="n">input_width</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">ow</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">oh</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">oc</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">z</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">z</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">ow</span> <span class="o">&lt;</span> <span class="n">out_width</span> <span class="o">&amp;&amp;</span> <span class="n">oh</span> <span class="o">&lt;</span> <span class="n">out_height</span> <span class="o">&amp;&amp;</span> <span class="n">oc</span> <span class="o">&lt;</span> <span class="n">out_channels</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">batch</span><span class="p">;</span> <span class="o">++</span><span class="n">b</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">ic</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">ic</span> <span class="o">&lt;</span> <span class="n">in_channels</span><span class="p">;</span> <span class="o">++</span><span class="n">ic</span><span class="p">)</span> <span class="p">{</span>
                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">kh</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">kh</span> <span class="o">&lt;</span> <span class="n">filter_height</span><span class="p">;</span> <span class="o">++</span><span class="n">kh</span><span class="p">)</span> <span class="p">{</span>
                    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">kw</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">kw</span> <span class="o">&lt;</span> <span class="n">filter_width</span><span class="p">;</span> <span class="o">++</span><span class="n">kw</span><span class="p">)</span> <span class="p">{</span>
                        <span class="kt">int</span> <span class="n">ih</span> <span class="o">=</span> <span class="n">oh</span> <span class="o">+</span> <span class="n">kh</span><span class="p">;</span>
                        <span class="kt">int</span> <span class="n">iw</span> <span class="o">=</span> <span class="n">ow</span> <span class="o">+</span> <span class="n">kw</span><span class="p">;</span>
                        <span class="k">if</span> <span class="p">(</span><span class="n">ih</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">ih</span> <span class="o">&lt;</span> <span class="n">input_height</span> <span class="o">&amp;&amp;</span> <span class="n">iw</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">iw</span> <span class="o">&lt;</span> <span class="n">input_width</span><span class="p">)</span> <span class="p">{</span>
                            <span class="n">sum</span> <span class="o">+=</span> <span class="n">input</span><span class="p">[((</span><span class="n">b</span> <span class="o">*</span> <span class="n">in_channels</span> <span class="o">+</span> <span class="n">ic</span><span class="p">)</span> <span class="o">*</span> <span class="n">input_height</span> <span class="o">+</span> <span class="n">ih</span><span class="p">)</span> <span class="o">*</span> <span class="n">input_width</span> <span class="o">+</span> <span class="n">iw</span><span class="p">]</span> <span class="o">*</span> 
                                   <span class="n">filter</span><span class="p">[((</span><span class="n">oc</span> <span class="o">*</span> <span class="n">in_channels</span> <span class="o">+</span> <span class="n">ic</span><span class="p">)</span> <span class="o">*</span> <span class="n">filter_height</span> <span class="o">+</span> <span class="n">kh</span><span class="p">)</span> <span class="o">*</span> <span class="n">filter_width</span> <span class="o">+</span> <span class="n">kw</span><span class="p">];</span>
                        <span class="p">}</span>
                    <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">}</span>
            <span class="n">output</span><span class="p">[((</span><span class="n">b</span> <span class="o">*</span> <span class="n">out_channels</span> <span class="o">+</span> <span class="n">oc</span><span class="p">)</span> <span class="o">*</span> <span class="n">out_height</span> <span class="o">+</span> <span class="n">oh</span><span class="p">)</span> <span class="o">*</span> <span class="n">out_width</span> <span class="o">+</span> <span class="n">ow</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="explanation">Explanation:</h3> <ul> <li><strong>Thread Calculation:</strong> Each thread calculates the output value for a specific position <code class="language-plaintext highlighter-rouge">(oh, ow)</code> in the output tensor.</li> <li><strong>Looping Over Channels:</strong> The kernel iterates over the input channels and filter dimensions to compute the convolution sum.</li> <li><strong>Output Assignment:</strong> The result is stored in the output tensor after summing over all contributions.</li> </ul> <h1 id="34-convolution-operation-on-cuda">3.4 Convolution Operation on CUDA</h1> <p>The convolution operation on CUDA involves mapping each element of the input tensor to the corresponding filter element and accumulating the result into the output tensor.</p> <ol> <li><strong>Thread-Block Mapping:</strong> <ul> <li>Each thread in the block is responsible for computing a single output element.</li> </ul> </li> <li><strong>Convolution Operation:</strong> <ul> <li>For each output element, the kernel iterates over the input channels and filter elements, performing the following computation:</li> </ul> </li> </ol> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">ih</span> <span class="o">=</span> <span class="n">oh</span> <span class="o">+</span> <span class="n">kh</span><span class="p">;</span> <span class="c1">// Input height index for convolution</span>
<span class="kt">int</span> <span class="n">iw</span> <span class="o">=</span> <span class="n">ow</span> <span class="o">+</span> <span class="n">kw</span><span class="p">;</span> <span class="c1">// Input width index for convolution</span>

<span class="k">if</span> <span class="p">(</span><span class="n">ih</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">ih</span> <span class="o">&lt;</span> <span class="n">input_height</span> <span class="o">&amp;&amp;</span> <span class="n">iw</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">iw</span> <span class="o">&lt;</span> <span class="n">input_width</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="n">input_val</span> <span class="o">=</span> <span class="n">input</span><span class="p">[((</span><span class="n">b</span> <span class="o">*</span> <span class="n">in_channels</span> <span class="o">+</span> <span class="n">ic</span><span class="p">)</span> <span class="o">*</span> <span class="n">input_height</span> <span class="o">+</span> <span class="n">ih</span><span class="p">)</span> <span class="o">*</span> <span class="n">input_width</span> <span class="o">+</span> <span class="n">iw</span><span class="p">];</span>
    <span class="kt">float</span> <span class="n">filter_val</span> <span class="o">=</span> <span class="n">filter</span><span class="p">[((</span><span class="n">oc</span> <span class="o">*</span> <span class="n">in_channels</span> <span class="o">+</span> <span class="n">ic</span><span class="p">)</span> <span class="o">*</span> <span class="n">filter_height</span> <span class="o">+</span> <span class="n">kh</span><span class="p">)</span> <span class="o">*</span> <span class="n">filter_width</span> <span class="o">+</span> <span class="n">kw</span><span class="p">];</span>
    <span class="n">sum</span> <span class="o">+=</span> <span class="n">input_val</span> <span class="o">*</span> <span class="n">filter_val</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <ol> <li><strong>Storing the Result:</strong> <ul> <li>The final convolution result is stored back in the output tensor:</li> </ul> </li> </ol> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span><span class="p">[((</span><span class="n">b</span> <span class="o">*</span> <span class="n">out_channels</span> <span class="o">+</span> <span class="n">oc</span><span class="p">)</span> <span class="o">*</span> <span class="n">out_height</span> <span class="o">+</span> <span class="n">oh</span><span class="p">)</span> <span class="o">*</span> <span class="n">out_width</span> <span class="o">+</span> <span class="n">ow</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
</code></pre></div></div> <h3 id="visualization-of-output-tensor">Visualization of Output Tensor</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output Tensor (1, 2, 3, 3):
-------------------------
| y | y | y |
| y | y | y |
| y | y | y |
-------------------------
| y | y | y |
| y | y | y |
| y | y | y |
-------------------------
</code></pre></div></div> <p>Each “y” represents the result of the convolution operation at that position in the output tensor, calculated by the corresponding CUDA thread.</p> <h1 id="4-nvidia-performance-metrics">4. NVIDIA Performance Metrics</h1> <p>Understanding and utilizing NVIDIA performance metrics is crucial for optimizing GPU-based convolution operations. These metrics provide insights into various aspects of GPU utilization and help identify bottlenecks in our implementation.</p> <h2 id="41-occupancy">4.1 Occupancy</h2> <p>Occupancy is a measure of how effectively we are keeping the GPU’s compute resources busy.</p> \[\text{Occupancy} = \frac{\text{Active Warps per SM}}{\text{Maximum Warps per SM}}\] <p>For convolution operations, high occupancy is generally desirable as it indicates efficient use of GPU resources. However, there can be trade-offs with other factors such as register usage and shared memory allocation.</p> <h2 id="42-memory-bandwidth-utilization">4.2 Memory Bandwidth Utilization</h2> <p>This metric measures how effectively we are using the GPU’s memory bandwidth.</p> \[\text{Memory Bandwidth Utilization} = \frac{\text{Actual Memory Throughput}}{\text{Theoretical Peak Memory Bandwidth}}\] <p>For convolution operations, which are often memory-bound, optimizing memory bandwidth utilization is critical. Techniques such as memory coalescing and efficient use of shared memory can significantly impact this metric.</p> <h2 id="43-compute-utilization">4.3 Compute Utilization</h2> <p>Compute utilization measures how effectively we are using the GPU’s arithmetic capabilities.</p> \[\text{Compute Utilization} = \frac{\text{Actual FLOPS}}{\text{Theoretical Peak FLOPS}}\] <p>In convolution operations, especially those with larger filter sizes, improving compute utilization can lead to significant performance gains.</p> <h2 id="44-instruction-throughput">4.4 Instruction Throughput</h2> <p>This metric measures how many instructions are executed per clock cycle.</p> \[\text{IPC (Instructions Per Cycle)} = \frac{\text{Number of Instructions Executed}}{\text{Number of Clock Cycles}}\] <p>For convolution kernels, optimizing instruction throughput often involves techniques like loop unrolling and minimizing branching.</p> <h2 id="45-warp-execution-efficiency">4.5 Warp Execution Efficiency</h2> <p>This metric indicates how efficiently the threads within a warp are being utilized.</p> \[\text{Warp Execution Efficiency} = \frac{\text{Average Active Threads per Warp}}{32} \times 100\%\] <p>In convolution operations, particularly at the edges of the input tensor, maintaining high warp execution efficiency can be challenging and may require special handling.</p> <h2 id="46-shared-memory-efficiency">4.6 Shared Memory Efficiency</h2> <p>This metric measures how effectively shared memory is being utilized.</p> \[\text{Shared Memory Efficiency} = \frac{\text{Shared Memory Throughput}}{\text{Theoretical Peak Shared Memory Throughput}}\] <p>Efficient use of shared memory is often key to optimizing convolution operations, as it can significantly reduce global memory accesses.</p> <h2 id="47-l1l2-cache-hit-rate">4.7 L1/L2 Cache Hit Rate</h2> <p>These metrics indicate how effectively the cache hierarchy is being utilized.</p> \[\text{L1 Cache Hit Rate} = \frac{\text{L1 Cache Hits}}{\text{Total Memory Accesses}}\] \[\text{L2 Cache Hit Rate} = \frac{\text{L2 Cache Hits}}{\text{Total Memory Accesses - L1 Cache Hits}}\] <p>For convolution operations, particularly those with spatial locality in memory access patterns, optimizing cache hit rates can lead to significant performance improvements.</p> <h2 id="48-roofline-model">4.8 Roofline Model</h2> <p>The Roofline model provides a visual representation of performance bottlenecks, plotting achievable performance against operational intensity.</p> \[\text{Operational Intensity} = \frac{\text{FLOPs}}{\text{Bytes Accessed}}\] \[\text{Attainable Performance} = \min(\text{Peak FLOPS}, \text{Operational Intensity} \times \text{Peak Memory Bandwidth})\] <p>For convolution operations, the Roofline model can help determine whether the kernel is compute-bound or memory-bound, guiding optimization efforts.</p> <p>In the next section, we will explore how these metrics can be applied to analyze and optimize specific aspects of convolution operations on GPUs.</p> <h1 id="5-performance-analysis-of-2d-convolution">5. Performance Analysis of 2D Convolution</h1> <p>In this section, we present a comprehensive analysis of various performance metrics for 2D convolution operations on GPUs. Each graph provides unique insights into the behavior and efficiency of the convolution kernels under different configurations.</p> <h2 id="51-execution-time-vs-block-configuration">5.1 Execution Time vs Block Configuration</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/execution_time_vs_block_configuration-480.webp 480w,/assets/img/execution_time_vs_block_configuration-800.webp 800w,/assets/img/execution_time_vs_block_configuration-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/execution_time_vs_block_configuration.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 1: Execution Time vs Block Configuration.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(T_{exec}(b) = f(b)\) where $b$ represents the block configuration and $f(b)$ is the execution time function.</p> <p><strong>Analysis:</strong> This graph illustrates how different block configurations affect the execution time of the convolution kernel. The goal is to minimize execution time. We observe that:</p> <ol> <li>Smaller block sizes (e.g., 16x16) often result in higher execution times due to underutilization of GPU resources.</li> <li>Larger block sizes (e.g., 32x32) generally reduce execution time but may plateau or increase beyond a certain point due to resource constraints.</li> <li>The optimal block size typically lies in the middle range, balancing resource utilization and scheduling overhead.</li> </ol> <h2 id="52-sm-efficiency-vs-block-configuration">5.2 SM Efficiency vs Block Configuration</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sm_efficiency_vs_block_configuration-480.webp 480w,/assets/img/sm_efficiency_vs_block_configuration-800.webp 800w,/assets/img/sm_efficiency_vs_block_configuration-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/sm_efficiency_vs_block_configuration.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 2: SM Efficiency vs Block Configuration.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(E_{SM}(b) = \frac{\text{Active SM Cycles}(b)}{\text{Total SM Cycles}(b)}\)</p> <p><strong>Analysis:</strong> This graph shows how efficiently the Streaming Multiprocessors (SMs) are utilized for different block configurations. Key observations include:</p> <ol> <li>Smaller block sizes often lead to lower SM efficiency due to insufficient parallelism.</li> <li>Larger block sizes generally increase SM efficiency up to a point, after which it may decrease due to resource contention.</li> <li>The block size that maximizes SM efficiency may not always correspond to the lowest execution time, highlighting the need for a balanced approach to optimization.</li> </ol> <h2 id="53-compute-throughput-vs-execution-time">5.3 Compute Throughput vs Execution Time</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/compute_throughput_vs_execution_time-480.webp 480w,/assets/img/compute_throughput_vs_execution_time-800.webp 800w,/assets/img/compute_throughput_vs_execution_time-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/compute_throughput_vs_execution_time.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 3: Compute Throughput vs Execution Time.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{Compute Throughput} = \frac{\text{Total FLOPs}}{\text{Execution Time}}\)</p> <p><strong>Analysis:</strong> This graph illustrates the relationship between compute throughput and execution time. Observations include:</p> <ol> <li>There’s often a trade-off between high compute throughput and low execution time.</li> <li>Configurations that achieve high throughput with relatively low execution time are ideal.</li> <li>The graph can help identify compute-bound vs. memory-bound configurations.</li> </ol> <h2 id="54-memory-throughput-vs-execution-time">5.4 Memory Throughput vs Execution Time</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/memory_throughput_vs_execution_time-480.webp 480w,/assets/img/memory_throughput_vs_execution_time-800.webp 800w,/assets/img/memory_throughput_vs_execution_time-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/memory_throughput_vs_execution_time.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 4: Memory Throughput vs Execution Time.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{Memory Throughput} = \frac{\text{Total Bytes Transferred}}{\text{Execution Time}}\)</p> <p><strong>Analysis:</strong> This graph shows the relationship between memory throughput and execution time. Key points:</p> <ol> <li>Higher memory throughput is generally desirable, but not at the cost of significantly increased execution time.</li> <li>Configurations that achieve high memory throughput with low execution time indicate efficient memory access patterns.</li> <li>The graph can help identify memory bottlenecks in the convolution kernel.</li> </ol> <h2 id="55-dram-vs-sm-frequency-analysis">5.5 DRAM vs SM Frequency Analysis</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dram_vs_sm_frequency_analysis-480.webp 480w,/assets/img/dram_vs_sm_frequency_analysis-800.webp 800w,/assets/img/dram_vs_sm_frequency_analysis-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/dram_vs_sm_frequency_analysis.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 5: DRAM vs SM Frequency Analysis.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{DRAM to SM Frequency Ratio} = \frac{\text{DRAM Frequency}}{\text{SM Frequency}}\)</p> <p><strong>Analysis:</strong> This graph compares DRAM throughput with SM frequency. Observations include:</p> <ol> <li>A balanced ratio indicates efficient utilization of both memory and compute resources.</li> <li>Imbalances can suggest either memory or compute bottlenecks in the convolution kernel.</li> <li>Optimal configurations maintain a balance between DRAM and SM utilization.</li> </ol> <h2 id="56-cache-hit-rate-analysis">5.6 Cache Hit Rate Analysis</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cache_hit_rate_analysis-480.webp 480w,/assets/img/cache_hit_rate_analysis-800.webp 800w,/assets/img/cache_hit_rate_analysis-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/cache_hit_rate_analysis.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 6: Cache Hit Rate Analysis.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{Cache Hit Rate} = \frac{\text{Cache Hits}}{\text{Total Memory Accesses}}\)</p> <p><strong>Analysis:</strong> This graph shows the cache hit rate for different configurations. Key points:</p> <ol> <li>Higher cache hit rates generally lead to better performance due to reduced DRAM accesses.</li> <li>The impact of cache hit rate on performance may vary depending on whether the kernel is compute-bound or memory-bound.</li> <li>Optimizing data layout and access patterns can significantly improve cache hit rates.</li> </ol> <h2 id="57-l1-cache-vs-l2-cache-throughput">5.7 L1 Cache vs L2 Cache Throughput</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/l1_cache_vs_l2_cache_throughput-480.webp 480w,/assets/img/l1_cache_vs_l2_cache_throughput-800.webp 800w,/assets/img/l1_cache_vs_l2_cache_throughput-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/l1_cache_vs_l2_cache_throughput.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 7: L1 Cache vs L2 Cache Throughput.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{L1 Cache Throughput} = f(\text{L1 Cache Accesses})\) \(\text{L2 Cache Throughput} = g(\text{L2 Cache Accesses})\)</p> <p><strong>Analysis:</strong> This graph compares the throughput of L1 and L2 caches. Observations include:</p> <ol> <li>Higher throughput for both L1 and L2 caches generally indicates more efficient memory access patterns.</li> <li>The balance between L1 and L2 cache usage can impact overall performance.</li> <li>Optimizing for L1 cache usage can significantly reduce memory latency in convolution operations.</li> </ol> <h2 id="58-sm-utilization-vs-memory-throughput">5.8 SM Utilization vs Memory Throughput</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sm_utilization_vs_memory_throughput-480.webp 480w,/assets/img/sm_utilization_vs_memory_throughput-800.webp 800w,/assets/img/sm_utilization_vs_memory_throughput-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/sm_utilization_vs_memory_throughput.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 8: SM Utilization vs Memory Throughput.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{SM Utilization} = \frac{\text{Active SM Time}}{\text{Total Execution Time}}\)</p> <p><strong>Analysis:</strong> This graph illustrates the relationship between SM utilization and memory throughput. Key observations:</p> <ol> <li>Higher SM utilization with higher memory throughput indicates efficient use of both compute and memory resources.</li> <li>Configurations in the upper-right quadrant of the graph are generally optimal, balancing compute and memory efficiency.</li> <li>Points clustered along the diagonal suggest a well-balanced kernel, while off-diagonal points may indicate bottlenecks.</li> </ol> <h2 id="59-achieved-warps-vs-occupancy">5.9 Achieved Warps vs Occupancy</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/achieved_warps_vs_occupancy-480.webp 480w,/assets/img/achieved_warps_vs_occupancy-800.webp 800w,/assets/img/achieved_warps_vs_occupancy-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/achieved_warps_vs_occupancy.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 9: Achieved Warps vs Occupancy.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{Occupancy} = \frac{\text{Achieved Warps}}{\text{Maximum Possible Warps}}\)</p> <p><strong>Analysis:</strong> This graph shows the relationship between achieved warps and occupancy. Observations include:</p> <ol> <li>Higher occupancy generally correlates with more achieved warps, indicating better resource utilization.</li> <li>The relationship may not be perfectly linear due to other limiting factors (e.g., shared memory usage, register pressure).</li> <li>Configurations that maximize both metrics are typically desirable for optimal performance.</li> </ol> <h2 id="510-performance-variability-analysis">5.10 Performance Variability Analysis</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/performance_variability_analysis-480.webp 480w,/assets/img/performance_variability_analysis-800.webp 800w,/assets/img/performance_variability_analysis-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/performance_variability_analysis.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 10: Performance Variability Analysis.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{Variability} = \frac{\text{Standard Deviation of Execution Time}}{\text{Mean Execution Time}}\)</p> <p><strong>Analysis:</strong> This graph illustrates the variability in execution time across different configurations. Key points:</p> <ol> <li>Lower variability is generally desirable for consistent performance.</li> <li>High variability may indicate sensitivity to factors like memory access patterns or thread divergence.</li> <li>Configurations with low variability and low execution time are ideal for stable, high-performance convolution operations.</li> </ol> <h2 id="511-memory-bandwidth-utilization-vs-block-configuration">5.11 Memory Bandwidth Utilization vs Block Configuration</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/memory_bandwidth_utilization_vs_block_configuration-480.webp 480w,/assets/img/memory_bandwidth_utilization_vs_block_configuration-800.webp 800w,/assets/img/memory_bandwidth_utilization_vs_block_configuration-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/memory_bandwidth_utilization_vs_block_configuration.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 11: Memory Bandwidth Utilization vs Block Configuration.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{Memory Bandwidth Utilization} = \frac{\text{Actual Memory Throughput}}{\text{Peak Memory Bandwidth}}\)</p> <p><strong>Analysis:</strong> This graph shows how effectively the available memory bandwidth is utilized for different block configurations. Observations:</p> <ol> <li>Higher utilization indicates better use of available bandwidth, crucial for memory-bound kernels.</li> <li>There’s often a sweet spot in block size that maximizes bandwidth utilization.</li> <li>Configurations with high bandwidth utilization but poor overall performance may indicate other bottlenecks.</li> </ol> <h2 id="512-register-pressure-analysis">5.12 Register Pressure Analysis</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/register_pressure_analysis-480.webp 480w,/assets/img/register_pressure_analysis-800.webp 800w,/assets/img/register_pressure_analysis-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/register_pressure_analysis.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 12: Register Pressure Analysis.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{Register Pressure} = \frac{\text{Registers Used per Thread}}{\text{Maximum Available Registers per Thread}}\)</p> <p><strong>Analysis:</strong> This graph illustrates the impact of register usage on performance. Key points:</p> <ol> <li>Higher register usage can limit the number of concurrent threads, potentially reducing occupancy.</li> <li>There’s often a trade-off between register usage and performance; some increase in register usage can improve performance by reducing memory accesses.</li> <li>The optimal point balances the benefits of additional registers against the cost of reduced occupancy.</li> </ol> <h2 id="513-elapsed-cycles-analysis">5.13 Elapsed Cycles Analysis</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/elapsed_cycles_analysis-480.webp 480w,/assets/img/elapsed_cycles_analysis-800.webp 800w,/assets/img/elapsed_cycles_analysis-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/elapsed_cycles_analysis.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 13: Elapsed Cycles Analysis.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{Elapsed Cycles} = \text{Clock Frequency} \times \text{Execution Time}\)</p> <p><strong>Analysis:</strong> This graph shows the total number of clock cycles elapsed during kernel execution for different configurations. Observations:</p> <ol> <li>Lower elapsed cycles generally indicate faster execution and better performance.</li> <li>The relationship between elapsed cycles and block configuration can reveal how effectively the GPU’s computational resources are being utilized.</li> <li>Configurations with low elapsed cycles but suboptimal performance may indicate other bottlenecks (e.g., memory bandwidth).</li> </ol> <h2 id="514-sm-active-cycles-vs-duration">5.14 SM Active Cycles vs Duration</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sm_active_cycles_vs_duration-480.webp 480w,/assets/img/sm_active_cycles_vs_duration-800.webp 800w,/assets/img/sm_active_cycles_vs_duration-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/sm_active_cycles_vs_duration.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 14: SM Active Cycles vs Duration.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{SM Efficiency} = \frac{\text{SM Active Cycles}}{\text{Total Cycles}}\)</p> <p><strong>Analysis:</strong> This graph compares the active cycles of SMs with the kernel execution duration. Key points:</p> <ol> <li>Points closer to the diagonal indicate higher SM efficiency, where most cycles are active cycles.</li> <li>Configurations with high active cycles but long durations may indicate memory or other bottlenecks.</li> <li>The ideal configuration maximizes active cycles while minimizing total duration.</li> </ol> <h2 id="515-compute-sm-throughput-vs-block-size">5.15 Compute SM Throughput vs Block Size</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/compute_sm_throughput_vs_block_size-480.webp 480w,/assets/img/compute_sm_throughput_vs_block_size-800.webp 800w,/assets/img/compute_sm_throughput_vs_block_size-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/compute_sm_throughput_vs_block_size.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 15: Compute SM Throughput vs Block Size.</div> </div> </div> <p><br/></p> <p><strong>Mathematical Representation:</strong> \(\text{Compute SM Throughput} = \frac{\text{Instructions Executed}}{\text{Execution Time} \times \text{Number of SMs}}\)</p> <p><strong>Analysis:</strong> This graph shows the relationship between compute throughput of SMs and the block size. Observations:</p> <ol> <li>Larger block sizes often result in higher throughput due to better utilization of SM resources.</li> <li>There’s typically a point of diminishing returns, after which increasing block size doesn’t significantly improve throughput.</li> <li>The optimal block size balances high throughput with other factors like occupancy and memory efficiency.</li> </ol> <p>These graphs provide a comprehensive view of the performance characteristics of 2D convolution operations on GPUs. By analyzing these metrics, developers can identify bottlenecks, optimize kernel configurations, and achieve better overall performance for convolution operations in deep learning and image processing applications.</p> <h1 id="6-key-takeaways-and-optimization-strategies">6. Key Takeaways and Optimization Strategies</h1> <h2 id="61-summary-of-key-insights">6.1 Summary of Key Insights</h2> <ol> <li> <p><strong>Block Configuration Impact</strong>: Block size significantly affects execution time, SM efficiency, and resource utilization. There’s often an optimal range that balances these factors.</p> </li> <li> <p><strong>Memory vs. Compute Balance</strong>: The relationship between memory throughput and compute throughput is crucial. Optimal performance often requires balancing these two aspects.</p> </li> <li> <p><strong>Cache Utilization</strong>: High cache hit rates, particularly for L1 cache, can significantly improve performance by reducing DRAM accesses.</p> </li> <li> <p><strong>Occupancy and Warp Execution</strong>: Higher occupancy generally correlates with better performance, but this relationship isn’t always linear due to other limiting factors.</p> </li> <li> <p><strong>Register Pressure</strong>: While using more registers can improve performance, excessive register usage can limit occupancy and overall performance.</p> </li> <li> <p><strong>SM Utilization</strong>: Maximizing SM active cycles while minimizing total execution time is key to efficient GPU utilization.</p> </li> <li> <p><strong>Memory Bandwidth</strong>: Effective utilization of memory bandwidth is crucial, especially for memory-bound convolution operations.</p> </li> </ol> <h2 id="62-optimization-strategies">6.2 Optimization Strategies</h2> <p>Based on these insights, we can formulate several optimization strategies for GPU-based convolution operations:</p> <h3 id="1-optimal-block-size-selection">1. Optimal Block Size Selection</h3> <p><strong>Strategy</strong>: Experiment with different block sizes to find the optimal configuration.</p> <p><strong>Implementation</strong>:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">dim3</span> <span class="nf">blockSize</span><span class="p">(</span><span class="n">BLOCK_SIZE_X</span><span class="p">,</span> <span class="n">BLOCK_SIZE_Y</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="kt">dim3</span> <span class="nf">gridSize</span><span class="p">((</span><span class="n">W_out</span> <span class="o">+</span> <span class="n">BLOCK_SIZE_X</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE_X</span><span class="p">,</span> 
              <span class="p">(</span><span class="n">H_out</span> <span class="o">+</span> <span class="n">BLOCK_SIZE_Y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">BLOCK_SIZE_Y</span><span class="p">,</span> 
              <span class="n">C_out</span><span class="p">);</span>
<span class="n">convolutionKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">gridSize</span><span class="p">,</span> <span class="n">blockSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">filter</span><span class="p">,</span> <span class="n">output</span><span class="p">);</span>
</code></pre></div></div> <p><strong>Rationale</strong>: The optimal block size balances SM efficiency, memory access patterns, and occupancy. It’s often problem-specific and requires empirical tuning.</p> <h3 id="2-tiling-and-shared-memory-utilization">2. Tiling and Shared Memory Utilization</h3> <p><strong>Strategy</strong>: Use shared memory to cache input data and filter weights.</p> <p><strong>Implementation</strong>:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">__shared__</span> <span class="kt">float</span> <span class="n">tile</span><span class="p">[</span><span class="n">TILE_SIZE</span><span class="p">][</span><span class="n">TILE_SIZE</span><span class="p">];</span>
<span class="c1">// Load data into shared memory</span>
<span class="c1">// Perform convolution using shared memory</span>
</code></pre></div></div> <p><strong>Rationale</strong>: Tiling reduces global memory accesses by reusing data loaded into shared memory, improving memory throughput and cache hit rates.</p> <h3 id="3-memory-coalescing">3. Memory Coalescing</h3> <p><strong>Strategy</strong>: Ensure global memory accesses are coalesced for efficient memory transactions.</p> <p><strong>Implementation</strong>:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Instead of:</span>
<span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">c</span><span class="p">][</span><span class="n">h</span><span class="p">][</span><span class="n">w</span><span class="p">];</span>

<span class="c1">// Use:</span>
<span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">input</span><span class="p">[(((</span><span class="n">b</span> <span class="o">*</span> <span class="n">C_in</span> <span class="o">+</span> <span class="n">c</span><span class="p">)</span> <span class="o">*</span> <span class="n">H_in</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span> <span class="o">*</span> <span class="n">W_in</span> <span class="o">+</span> <span class="n">w</span><span class="p">)];</span>
</code></pre></div></div> <p><strong>Rationale</strong>: Coalesced memory accesses maximize memory bandwidth utilization, crucial for memory-bound convolution operations.</p> <h3 id="4-loop-unrolling-and-instruction-level-optimization">4. Loop Unrolling and Instruction-Level Optimization</h3> <p><strong>Strategy</strong>: Unroll loops to increase instruction-level parallelism.</p> <p><strong>Implementation</strong>:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#pragma unroll
</span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">FILTER_SIZE</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Convolution computation</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Rationale</strong>: Loop unrolling can increase SM utilization and instruction throughput, potentially improving performance for compute-bound scenarios.</p> <h3 id="5-register-pressure-management">5. Register Pressure Management</h3> <p><strong>Strategy</strong>: Carefully manage register usage to balance performance and occupancy.</p> <p><strong>Implementation</strong>:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__launch_bounds__</span><span class="p">(</span><span class="n">MAX_THREADS_PER_BLOCK</span><span class="p">,</span> <span class="n">MIN_BLOCKS_PER_SM</span><span class="p">)</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="n">convolutionKernel</span><span class="p">(...)</span>
</code></pre></div></div> <p><strong>Rationale</strong>: Proper register management ensures high occupancy while providing enough registers for efficient computation.</p> <h3 id="6-kernel-fusion">6. Kernel Fusion</h3> <p><strong>Strategy</strong>: Fuse multiple operations (e.g., convolution + activation) into a single kernel.</p> <p><strong>Implementation</strong>:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolutionActivationKernel</span><span class="p">(...)</span>
<span class="p">{</span>
    <span class="c1">// Perform convolution</span>
    <span class="c1">// Immediately apply activation function</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Rationale</strong>: Kernel fusion reduces memory bandwidth requirements and kernel launch overhead, potentially improving overall performance.</p> <h3 id="7-mixed-precision-arithmetic">7. Mixed Precision Arithmetic</h3> <p><strong>Strategy</strong>: Use lower precision (e.g., FP16) where accuracy allows.</p> <p><strong>Implementation</strong>:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;cuda_fp16.h&gt;</span><span class="cp">
</span><span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolutionKernel</span><span class="p">(</span><span class="n">half</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="n">half</span><span class="o">*</span> <span class="n">filter</span><span class="p">,</span> <span class="n">half</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span>
<span class="p">{</span>
    <span class="c1">// Convolution using half-precision arithmetic</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Rationale</strong>: Lower precision arithmetic can increase computational throughput and reduce memory bandwidth requirements.</p> <h2 id="63-performance-modeling-and-autotuning">6.3 Performance Modeling and Autotuning</h2> <p>To systematically optimize convolution operations, consider implementing:</p> <ol> <li> <p><strong>Analytical Performance Model</strong>: Develop a model that predicts performance based on kernel parameters and hardware characteristics.</p> </li> <li> <p><strong>Autotuning Framework</strong>: Create a system that automatically explores the parameter space (block size, tiling strategy, etc.) to find optimal configurations.</p> </li> <li> <p><strong>Profile-Guided Optimization</strong>: Use profiling data to guide optimizations, focusing efforts on the most impactful areas of the convolution kernel.</p> </li> </ol> <p>By applying these strategies and continuously analyzing performance metrics, developers can significantly improve the efficiency of GPU-based convolution operations, leading to faster and more efficient deep learning and image processing applications.</p> <d-footnote>Thank you for exploring Conv2d with me.</d-footnote>]]></content><author><name>GPU Optimization Researcher</name></author><summary type="html"><![CDATA[A comprehensive analysis of convolution operations on GPUs, focusing on theoretical foundations, performance metrics, and optimization strategies.]]></summary></entry><entry><title type="html">Analytical Exploration of Riemann Surfaces</title><link href="https://alphapibeta.github.io/blog/2024/Riemann-Surface/" rel="alternate" type="text/html" title="Analytical Exploration of Riemann Surfaces"/><published>2024-05-23T00:00:00+00:00</published><updated>2024-05-23T00:00:00+00:00</updated><id>https://alphapibeta.github.io/blog/2024/Riemann-Surface</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2024/Riemann-Surface/"><![CDATA[<h2 id="overview">Overview</h2> <p>Riemann surfaces are complex one-dimensional manifolds pivotal in complex analysis, providing a natural domain for defining and studying holomorphic functions that transcend the simple complex plane. These surfaces are essential in connecting diverse mathematical disciplines, such as topology, geometry, and complex analysis, and serve as a platform for multi-valued function integration.</p> <h3 id="broader-perspectives">Broader Perspectives</h3> <p>Riemann surfaces play a crucial role in topology and projective geometry. In topology, they clarify the properties of surfaces, linking abstract mathematical theories with more concrete geometric forms. Projective geometry views these surfaces within projective spaces, exploring their symmetries and geometric properties. Additionally, geometric algebra provides a framework to analyze their deeper algebraic structures, enhancing our understanding of their complex interactions.</p> <h3 id="focus-of-this-blog">Focus of This Blog</h3> <p>This blog concentrates on the complex analysis of Riemann surfaces, specifically how they aid in understanding holomorphic functions. We leverage foundational complex variable theories while recognizing that projective geometry and geometric algebra offer valuable insights into their broader mathematical context.</p> <h2 id="foundations-of-holomorphy">Foundations of Holomorphy</h2> <p>To understand Riemann surfaces, we recall the essential characteristics of holomorphic functions:</p> <p>A function \(( f(z) )\) is said to be <strong>holomorphic</strong> at a point \(( z_0 )\) if it satisfies the following conditions:</p> <ol> <li> <p><strong>Complex Differentiability</strong>:</p> <ul> <li>\(( f )\) is differentiable at \(( z_0 )\), and the derivative \(( f'(z_0) )\) exists.</li> <li>The limit \((\lim_{z \to z_0} \frac{f(z) - f(z_0)}{z - z_0})\) exists.</li> </ul> </li> <li> <p><strong>Analyticity</strong>: \(( f )\) can be represented by a convergent power series in the neighborhood of \(( z_0 )\):</p> \[f(z) = \sum_{n=0}^{\infty} a_n (z - z_0)^n\] </li> <li> <p><strong>Cauchy-Riemann Equations</strong>:</p> <ul> <li>For \(( f )\) expressed as \(( u(x, y) + iv(x, y) )\), where \(( z = x + iy )\), it must satisfy: \(\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}, \quad \frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x}\)</li> </ul> </li> </ol> <h2 id="local-representation-in-holomorphic-functions-on-riemann-surfaces">Local Representation in Holomorphic Functions on Riemann Surfaces</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Riemann-01-480.webp 480w,/assets/img/Riemann-01-800.webp 800w,/assets/img/Riemann-01-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Riemann-01.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 1: Local Representation in Holomorphic Functions on Riemann Surfaces.</div> </div> </div> <p><br/></p> <ul> <li>$D \subseteq X$ represents the neighborhood on the Riemann surface around point $x_0$.</li> <li>$\Delta \subseteq \mathbb{C}$ represents a disk in the complex plane.</li> <li>$\phi$ is the homeomorphism from $D$ to $\Delta$.</li> <li> <p>$f \circ \phi^{-1}$ maps from $\Delta$ back to $\mathbb{C}$, facilitating the analysis of $f$ as if it were defined in the complex plane.</p> </li> <li>In the context of Riemann surfaces, local representation plays a pivotal role in understanding how complex functions can be analyzed and manipulated in settings that extend beyond the traditional complex plane. This concept is particularly valuable when dealing with the local properties of functions defined on complex manifolds like Riemann surfaces.</li> </ul> <h3 id="conceptual-explanation">Conceptual Explanation</h3> <ul> <li> <p>For any given point \(x_0\) on a Riemann surface \(X\), we can identify a neighborhood \(D\) around \(x_0\). This neighborhood can be mapped to a simpler, well-understood domain within the complex plane \(\mathbb{C}\), typically a unit disk \(\Delta\). This mapping is achieved through a homeomorphism \(\phi\), which translates the complexities of the surface’s local structure into the Euclidean domain of \(\mathbb{C}\).</p> </li> <li> <p>The function \(f: D \rightarrow \mathbb{C}\) defined on \(X\) is then analyzed by considering its behavior when transported via \(\phi\) to \(\Delta\). This transposition allows for the application of established complex analysis techniques, as \(f\) can be treated similarly to functions directly defined on \(\mathbb{C}\).</p> </li> </ul> <p>The transformation of \(f\) through \(\phi\) and its subsequent analysis can be formally expressed as follows:</p> <ul> <li><strong>Mapping Definition:</strong> The homeomorphism \(\phi: D \rightarrow \Delta\) maps the neighborhood \(D\) around \(x_0\) to a unit disk \(\Delta\) in the complex plane.</li> <li><strong>Function Transformation:</strong> The function \(f\) is then considered in terms of \(\phi\), leading to the transformed function \(f \circ \phi^{-1}\), which operates within \(\Delta\).</li> </ul> <p>This process is critical for determining if \(f\) is holomorphic at \(x_0\). Holomorphy is assessed by checking if \(f \circ \phi^{-1}\) meets the criteria for holomorphic functions within the complex domain of \(\Delta\).</p> <h2 id="transition-maps-and-holomorphicity-across-charts">Transition Maps and Holomorphicity Across Charts</h2> <h3 id="coordinate-charts-and-their-role">Coordinate Charts and Their Role</h3> <p>Coordinate charts provide a methodological approach to treating Riemann surfaces as cohesive entities amenable to complex analysis. A coordinate chart on a Riemann surface $X$ is defined as a pair $(U, \phi)$ where $U$ is an open subset of $X$, and $\phi$ is a homeomorphism mapping $U$ onto an open subset of $\mathbb{C}$, typically visualized as a disk or similar construct.</p> <p>This identification allows functions defined on the complex surface to be analyzed as if they were functions on the complex plane, greatly simplifying their study and manipulation under classical complex analysis techniques.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Riemann-02-480.webp 480w,/assets/img/Riemann-02-800.webp 800w,/assets/img/Riemann-02-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Riemann-02.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 2: Coordinate Charts and Their Role.</div> </div> </div> <p><br/></p> <h3 id="preliminary-definition-of-riemann-surfaces">Preliminary Definition of Riemann Surfaces</h3> <p>Riemann surfaces can initially be described as surfaces covered by a collection of charts $(U, \phi)$, where each chart encapsulates a portion of the surface mapped homomorphically onto an open subset of $\mathbb{C}$. This setup highlights a potential complication: \(X = \bigcup_{\alpha \in I} U_{\alpha}\)</p> <p>Where $I$ is an indexing set for the charts. The primary concern arises when defining holomorphic functions across these charts, particularly when a point $x_0$ appears in multiple overlapping charts. The definition of holomorphy must be consistent regardless of the chart, emphasizing the function’s intrinsic properties rather than its representation under any specific coordinate system.</p> <p>By defining transition functions $g_{\alpha \beta} = \phi_\beta \circ \phi_\alpha^{-1}$ that are holomorphic, the structure ensures that holomorphicity is a consistent property across the entire surface, regardless of the local coordinate representation. This foundational aspect solidifies the concept of Riemann surfaces as true complex manifolds, where local complexities are globally harmonized through sophisticated mathematical frameworks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Riemann-03-480.webp 480w,/assets/img/Riemann-03-800.webp 800w,/assets/img/Riemann-03-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Riemann-03.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 3: Preliminary Definition of Riemann Surfaces.</div> </div> </div> <p><br/></p> <h3 id="transition-maps">Transition Maps</h3> <p>Transition maps are essential mechanisms within the framework of Riemann surfaces, ensuring that the manifold structure robustly supports complex analysis across its expanse. These maps are particularly critical when dealing with overlapping charts on the surface, such as $(U, \phi)$ and $(V, \psi)$. The transition map, denoted by $\psi \circ \phi^{-1}$, plays a pivotal role in maintaining the consistency of the complex structure:</p> \[\psi \circ \phi^{-1}: \phi(U \cap V) \rightarrow \psi(U \cap V)\] <p>This mathematical requirement guarantees that the complex structure is preserved across different regions of the surface. It ensures that functions recognized as holomorphic within one chart exhibit the same properties in all overlapping charts, thereby upholding a uniform analytic structure across the surface.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Riemann-04-480.webp 480w,/assets/img/Riemann-04-800.webp 800w,/assets/img/Riemann-04-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Riemann-04.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 4: Transition Maps and Holomorphicity Across Charts on Riemann Surfaces.</div> </div> </div> <p><br/></p> <h3 id="holomorphicity-across-charts">Holomorphicity Across Charts</h3> <p>The holomorphy of functions on Riemann surfaces is deeply tied to their performance under these coordinate transformations. To qualify a function $f$ as holomorphic across a Riemann surface, it must show consistent holomorphic behavior under the transformation by each chart’s mapping. Specifically, if $f$ is holomorphic at a point $x_0$ in a chart $(U, \phi)$, then for any other chart $(V, \psi)$ that overlaps with $U$ at $x_0$, the function must be holomorphic when transformed by both mappings:</p> <ul> <li>$(f \circ \phi^{-1})$ must be holomorphic on $\phi(U \cap V)$</li> <li>$(f \circ \psi^{-1})$ must be holomorphic on $\psi(U \cap V)$</li> </ul> <p>These conditions emphasize the integral nature of holomorphy as a characteristic that transcends local chart boundaries and becomes a global property of functions on Riemann surfaces. By ensuring that these transition maps and transformations maintain holomorphic integrity, Riemann surfaces facilitate a broader and more profound application of complex analysis, extending classical techniques into more complex topological structures.</p> <h4 id="transition-function-definition">Transition Function Definition:</h4> <p>The transition function is defined as: \(g*{12} = (\phi*{\alpha*1}^{-1} |*{U*1 \cap U_2}) \circ (\phi*{\alpha*2} |*{V\_{\alpha21}})\) This function maps from one local chart to another through the overlap of their respective domains, where \(( U_1 )\) and \(( U_2 )\) are overlapping subsets of charts \(( \alpha_1 )\) and \(( \alpha_2 )\).</p> <h4 id="homeomorphism-and-open-sets">Homeomorphism and Open Sets:</h4> <table> <tbody> <tr> <td>Both components $$( (\phi_{\alpha_1}^{-1}</td> <td><em>{U_1 \cap U_2}) )\(and\)( (\phi</em>{\alpha_2}</td> <td><em>{V</em>{\alpha21}}) )$$ represent open subsets, ensuring the transition map functions as a homeomorphism. This is crucial for maintaining the integrity of the Riemann surface’s structure.</td> </tr> </tbody> </table> <h4 id="holomorphic-isomorphism">Holomorphic Isomorphism:</h4> <p>To maintain the holomorphic nature of functions, the transition function \(( g_{12} )\) must not only be a homeomorphism but also a holomorphic isomorphism. This means it should preserve the complex structure, ensuring that the function is injective and remains holomorphic across the transition:</p> \[(f \circ \phi*{\alpha_1}^{-1}) \circ g*{12} = f \circ \phi\_{\alpha_2}^{-1}\] <p>This equation demonstrates that the function \(( f )\) maintains its holomorphic properties across overlapping charts, effectively showing that the holomorphic nature of \(( f )\) is preserved through the mappings from one chart to another via the transition map.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Riemann-05-480.webp 480w,/assets/img/Riemann-05-800.webp 800w,/assets/img/Riemann-05-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Riemann-05.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 5: Summary.</div> </div> </div> <p><br/></p> <h2 id="conclusions">Conclusions</h2> <p>In this introduction, we delved into the complex analysis of Riemann surfaces, starting from the basic definitions and moving towards more intricate properties of functions defined on these surfaces. By identifying and examining neighborhoods around specific points, such as $(X_0)$, we began to unravel the fundamental characteristics of holomorphic functions within these local environments. Additionally, we explored how coordinate charts serve as essential tools for translating the complex topological structure of Riemann surfaces into manageable subsets of the complex plane. These charts, while localized around neighborhoods, crucially link the local geometric properties to the broader analytical framework, allowing us to extend complex analysis techniques to the realm of Riemann surfaces effectively.</p> <h3 id="final-thoughts">Final Thoughts</h3> <p>Riemann surfaces offer deep insights into complex functions, linking concepts across topology, geometry, and complex analysis, with promising applications in machine learning and deep learning. Their complex mappings and topological properties are analogous to the transformations seen in high-dimensional neural networks, providing inspiration for new algorithmic strategies and neural architectures.</p> <p>For example, the holomorphic properties central to Riemann surfaces could influence the development of novel activation functions or optimization techniques in deep learning. Additionally, perspectives from projective geometry and geometric algebra enrich our approach to algorithm design, suggesting innovative ways to structure data and transformations in AI systems.</p> <d-cite key="Riemann_surface"></d-cite> <d-footnote>Thank you for exploring Riemann surfaces with me.</d-footnote>]]></content><author><name>Ronak Haresh Chhatbar</name></author><category term="Mathematics,"/><category term="Complex-Analysis,"/><category term="Riemann-Surfaces"/><summary type="html"><![CDATA[This introduction explores the application of complex analysis on Riemann surfaces, focusing on defining neighborhoods and understanding the properties of functions, coordinate charts translation maps, providing a foundation for extending complex analytical methods to Riemann surfaces.]]></summary></entry><entry><title type="html">FlowNet Learning Optical Flow with Convolutional Networks</title><link href="https://alphapibeta.github.io/blog/2024/flownet/" rel="alternate" type="text/html" title="FlowNet Learning Optical Flow with Convolutional Networks"/><published>2024-01-30T00:00:00+00:00</published><updated>2024-01-30T00:00:00+00:00</updated><id>https://alphapibeta.github.io/blog/2024/flownet</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2024/flownet/"><![CDATA[<h2 id="abstract">Abstract</h2> <p><strong>Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.</strong></p> <h2 id="introduction">Introduction</h2> <blockquote> <p><strong>optical flow estimation needs precise per-pixel localization, it also requires finding correspondences between two input images. This involves not only learning image feature representations, but also learning to match them at different locations in the two images. In this respect, optical flow estimation fundamentally differs from previous applications of CNNs.</strong></p> </blockquote> <div class="row mt-10"> <div class="col-md-10 col-sm-10 mt-10 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flownet01-480.webp 480w,/assets/img/flownet01-800.webp 800w,/assets/img/flownet01-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/flownet01.png" class="img-fluid rounded z-depth-4 img-smaller" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 1:Efficacy of Correlation Layer in CNNs for Optical Flow Prediction.</div> </div> </div> <p>The network intorduces a new layer called <code class="language-plaintext highlighter-rouge">correlation layer</code> that significantly improves optical flow estimation by efficiently finding correspondences between different image patches.</p> <h2 id="network-architectures">Network Architectures</h2> <p>This paper takes an end-to-end learning approach to predicting optical flow: given a dataset consisting of image pairs and ground truth flows, we train a network to predict the x–y flow fields directly from the images.</p> <p><code class="language-plaintext highlighter-rouge">Pooling in CNNs is necessary to make network training computationally feasible and, more fundamentally, to allow aggregation of information over large areas of the input images</code></p> <p>Networks consisting of <code class="language-plaintext highlighter-rouge">contracting</code> and <code class="language-plaintext highlighter-rouge">expanding</code> parts are trained as a whole using backpropagation. Architectures we use are depicted in Figures 3 and 4.</p> <h2 id="contracting-part"><strong>Contracting part</strong></h2> <ul> <li> <p>A simple choice is to stack both input images together and feed them through a rather generic network, allowing the network to decide itself how to process Figure 2. Refinement of the coarse feature maps to the high resothe image pair to extract the motion information. This is illution prediction. lustrated in Figure 3.</p> </li> <li> <p>Another approach is to create two separate, yet identical processing streams for the two images and to combine them at a later stage as shown in Fig. 3. With this architecture the network is constrained to first produce meaningful representations of the two images separately and then combine them on a higher level</p> </li> </ul> <h3 id="correlation-layer-in-flownet">Correlation Layer in FlowNet</h3> <p>The correlation layer is a pivotal component of FlowNet’s architecture, allowing the network to compare patches from feature maps \(( f_1 )\) and \(( f_2 )\) derived from two input images. This comparison is fundamental for optical flow estimation, as it involves finding correspondences between different locations in these images.</p> <h4 id="mathematical-formulation">Mathematical Formulation</h4> <p>The correlation of two patches centered at \(( x_1 )\)in the first map and \(( x_2 )\) in the second map is defined as:</p> \[c(x_1, x_2) = \sum_{o \in [-k, k] \times [-k, k]} \langle f_1(x_1 + o), f_2(x_2 + o) \rangle\] <p>where \(( \langle \cdot, \cdot \rangle )\) denotes the scalar product, and the summation is over a square patch of size \(( K := 2k + 1 )\). This equation is similar to a convolution operation but involves convolving data with other data, not with a filter.</p> <h4 id="computational-considerations">Computational Considerations</h4> <p>For each location \(( x_1 )\), the correlations \(( c(x_1, x_2) )\) are computed only in a neighborhood of size \(( D := 2d + 1 )\), where \(( d )\) is the maximum displacement allowed. This is to make the computation tractable, as comparing all possible patch combinations without this limitation would be computationally expensive.</p> <p>The computational cost of computing \(( c(x_1, x_2) )\) involves \(( c \cdot K^2 )\) multiplications, and the total cost for all patch combinations is proportional to \(( w^2 \cdot h^2 )\), where \(( w )\) and \(( h )\) are the width and height of the feature maps.</p> <h4 id="implementation">Implementation</h4> <p>In practice, the output of the correlation layer is organized into channels, representing relative displacements. This results in an output size of \(( (w \times h \times D^2) )\). For the backward pass, derivatives are implemented with respect to each input feature map.</p> <p>estimates the computational complexity for computing feature map correlations, factoring in patch size (K), dimensions (w and h), and channel count (c) of the feature maps.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flownet02-480.webp 480w,/assets/img/flownet02-800.webp 800w,/assets/img/flownet02-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/flownet02.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 2: Flownet simple.</div> </div> </div> <p><br/></p> <p><code class="language-plaintext highlighter-rouge">If we notice the architecture we see that the images are stacked together channel wise.</code></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flownet03-480.webp 480w,/assets/img/flownet03-800.webp 800w,/assets/img/flownet03-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/flownet03.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 3: Flownet Corelation.</div> </div> </div> <h2 id="expanding-part"><strong>Expanding part</strong></h2> <p>The main ingredient of the expanding part are <code class="language-plaintext highlighter-rouge">upconvolutional layers</code>, consisting of unpooling (extending the feature maps, as opposed to pooling) and a convolution. Such layers have been used previously [38, 37, 16, 28, 9]. To perform the refinement, we apply the ‘upconvolution’ to feature maps, and concatenate it with corresponding feature maps from the ’contractive’ part of the network and an upsampled coarser flow prediction (if available). This way we preserve both the high-level information passed from coarser feature maps and fine local information provided in lower layer feature maps. Each step increases the resolution twice. We repeat this 4 times, resulting in a predicted flow for which the resolution is still 4 times smaller than the input.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flownet04-480.webp 480w,/assets/img/flownet04-800.webp 800w,/assets/img/flownet04-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/flownet04.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 4: refinement-layrer.</div> </div> </div> <h2 id="datasets">Datasets</h2> <p><strong>Flying Chairs The Sintel dataset is still too small to train large CNNs. To provide enough training data, we create a simple syn- thetic dataset, which we name Flying Chairs, by applying affine transformations to images collected from Flickr and a publicly available set of renderings of 3D chair models [1]. We retrieve 964 images from Flickr2 with a resolution of 1, 024 × 768 from the categories ‘city’ (321), ‘landscape’ (129) and ‘mountain’ (514). We cut the images into 4 quad- rants and use the resulting 512 × 384 image crops as back- ground. As foreground objects we add images of multi- ple chairs from [1] to the background. From the original dataset we remove very similar chairs, resulting in 809 chair types and 62 views per chair available. Examples are shown in Figure 5. To generate motion, we randomly sample 2D affine transformation parameters for the background and the chairs. The chairs’ transformations are relative to the back- ground transformation, which can be interpreted as both the camera and the objects moving. Using the transformation parameters we generate the second image, the ground truth optical flow and occlusion regions.</strong></p> <h2 id="results">Results</h2> <table> <thead> <tr> <th style="text-align: left">Method</th> <th style="text-align: right">Sintel Clean train</th> <th style="text-align: right">Sintel Clean test</th> <th style="text-align: right">Sintel Final train</th> <th style="text-align: right">Sintel Final test</th> <th style="text-align: right">KITTI train</th> <th style="text-align: right">KITTI test</th> <th style="text-align: right">Middlebury train</th> <th style="text-align: right">Middlebury test</th> <th style="text-align: right">Chairs test</th> <th style="text-align: right">Time (sec) CPU</th> <th style="text-align: right">Time (sec) GPU</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">EpicFlow [30]</td> <td style="text-align: right">2.27</td> <td style="text-align: right">4.12</td> <td style="text-align: right">3.57</td> <td style="text-align: right">6.29</td> <td style="text-align: right">3.47</td> <td style="text-align: right">3.8</td> <td style="text-align: right">0.31</td> <td style="text-align: right">3.55</td> <td style="text-align: right">2.94</td> <td style="text-align: right">16</td> <td style="text-align: right">-</td> </tr> <tr> <td style="text-align: left">DeepFlow [35]</td> <td style="text-align: right">3.19</td> <td style="text-align: right">5.38</td> <td style="text-align: right">4.40</td> <td style="text-align: right">7.21</td> <td style="text-align: right">4.58</td> <td style="text-align: right">5.8</td> <td style="text-align: right">0.21</td> <td style="text-align: right">4.22</td> <td style="text-align: right">3.53</td> <td style="text-align: right">17</td> <td style="text-align: right">-</td> </tr> <tr> <td style="text-align: left">EPPM [3]</td> <td style="text-align: right">-</td> <td style="text-align: right">6.49</td> <td style="text-align: right">-</td> <td style="text-align: right">8.38</td> <td style="text-align: right">-</td> <td style="text-align: right">9.2</td> <td style="text-align: right">-</td> <td style="text-align: right">3.36</td> <td style="text-align: right">-</td> <td style="text-align: right">-</td> <td style="text-align: right">0.2</td> </tr> <tr> <td style="text-align: left">LDOF [6]</td> <td style="text-align: right">4.19</td> <td style="text-align: right">7.56</td> <td style="text-align: right">6.28</td> <td style="text-align: right">9.12</td> <td style="text-align: right">13.73</td> <td style="text-align: right">12.4</td> <td style="text-align: right">0.45</td> <td style="text-align: right">4.55</td> <td style="text-align: right">3.47</td> <td style="text-align: right">65</td> <td style="text-align: right">2.5</td> </tr> <tr> <td style="text-align: left">FlowNetS</td> <td style="text-align: right">4.50</td> <td style="text-align: right">7.42</td> <td style="text-align: right">5.45</td> <td style="text-align: right">8.43</td> <td style="text-align: right">8.26</td> <td style="text-align: right">-</td> <td style="text-align: right">1.09</td> <td style="text-align: right">-</td> <td style="text-align: right">2.71</td> <td style="text-align: right">-</td> <td style="text-align: right">0.08</td> </tr> <tr> <td style="text-align: left">FlowNetS+v</td> <td style="text-align: right">3.66</td> <td style="text-align: right">6.45</td> <td style="text-align: right">4.76</td> <td style="text-align: right">7.67</td> <td style="text-align: right">6.50</td> <td style="text-align: right">-</td> <td style="text-align: right">0.33</td> <td style="text-align: right">-</td> <td style="text-align: right">2.86</td> <td style="text-align: right">-</td> <td style="text-align: right">1.05</td> </tr> <tr> <td style="text-align: left">FlowNetS+ft</td> <td style="text-align: right">6.96</td> <td style="text-align: right">7.76</td> <td style="text-align: right">7.76</td> <td style="text-align: right">7.22</td> <td style="text-align: right">7.52</td> <td style="text-align: right">9.1</td> <td style="text-align: right">15.20</td> <td style="text-align: right">-</td> <td style="text-align: right">3.04</td> <td style="text-align: right">-</td> <td style="text-align: right">0.08</td> </tr> <tr> <td style="text-align: left">FlowNetS+ft+v</td> <td style="text-align: right">6.16</td> <td style="text-align: right">7.22</td> <td style="text-align: right">7.22</td> <td style="text-align: right">7.22</td> <td style="text-align: right">6.07</td> <td style="text-align: right">7.6</td> <td style="text-align: right">3.84</td> <td style="text-align: right">0.47</td> <td style="text-align: right">4.58</td> <td style="text-align: right">-</td> <td style="text-align: right">1.05</td> </tr> <tr> <td style="text-align: left">FlowNetC</td> <td style="text-align: right">4.31</td> <td style="text-align: right">7.28</td> <td style="text-align: right">5.87</td> <td style="text-align: right">8.81</td> <td style="text-align: right">9.35</td> <td style="text-align: right">-</td> <td style="text-align: right">15.64</td> <td style="text-align: right">-</td> <td style="text-align: right">2.19</td> <td style="text-align: right">-</td> <td style="text-align: right">0.15</td> </tr> <tr> <td style="text-align: left">FlowNetC+v</td> <td style="text-align: right">3.57</td> <td style="text-align: right">6.27</td> <td style="text-align: right">5.25</td> <td style="text-align: right">8.01</td> <td style="text-align: right">7.45</td> <td style="text-align: right">-</td> <td style="text-align: right">3.92</td> <td style="text-align: right">-</td> <td style="text-align: right">2.61</td> <td style="text-align: right">-</td> <td style="text-align: right">1.12</td> </tr> <tr> <td style="text-align: left">FlowNetC+ft</td> <td style="text-align: right">6.85</td> <td style="text-align: right">8.51</td> <td style="text-align: right">8.51</td> <td style="text-align: right">7.88</td> <td style="text-align: right">7.31</td> <td style="text-align: right">-</td> <td style="text-align: right">12.33</td> <td style="text-align: right">-</td> <td style="text-align: right">2.27</td> <td style="text-align: right">-</td> <td style="text-align: right">0.15</td> </tr> <tr> <td style="text-align: left">FlowNetC+ft+v</td> <td style="text-align: right">6.08</td> <td style="text-align: right">7.88</td> <td style="text-align: right">7.88</td> <td style="text-align: right">7.88</td> <td style="text-align: right">6.07</td> <td style="text-align: right">7.6</td> <td style="text-align: right">3.81</td> <td style="text-align: right">0.50</td> <td style="text-align: right">4.52</td> <td style="text-align: right">-</td> <td style="text-align: right">1.12</td> </tr> </tbody> </table> <p>This Table represents Average endpoint errors (in pixels) of our networks compared to several well-performing methods on different datasets. The numbers in parentheses are the results of the networks on data they were trained on, and hence are not directly comparable to other results.</p> <ul> <li> <p><code class="language-plaintext highlighter-rouge">FlowNetS</code> generalizes better to datasets with additional complexities such as motion blur or fog, as evidenced by its performance on the Sintel Final dataset. In contrast, <code class="language-plaintext highlighter-rouge">FlowNetC</code> performs better on datasets with fewer complexities, such as Flying Chairs and Sintel Clean, indicating a proclivity to overfit to the type of data encountered during training. Despite having similar numbers of parameters, <code class="language-plaintext highlighter-rouge">FlowNetC's</code> overfitting may be advantageous with better training data.</p> </li> <li> <p>Furthermore, <code class="language-plaintext highlighter-rouge">FlowNetC</code> appears to struggle more with large displacements than <code class="language-plaintext highlighter-rouge">FlowNetS</code>, as demonstrated by its performance on the KITTI dataset and detailed analysis on Sintel Final. The constraint on maximum displacement within <code class="language-plaintext highlighter-rouge">FlowNetC's</code> correlation layer may be the limiting factor, which can be adjusted at the expense of computational efficiency.</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <p><strong>Building on recent progress in design of convolutional network architectures, we have shown that it is possible to train a network to directly predict optical flow from two in- put images. Intriguingly, the training data need not be re- alistic. The artificial Flying Chairs dataset including just affine motions of synthetic rigid objects is sufficient to pre- dict optical flow in natural scenes with competitive accu- racy. This proves the generalization capabilities of the pre- sented networks. On the test set of the Flying Chairs the CNNs even outperform state-of-the-art methods like Deep- Flow and EpicFlow. It will be interesting to see how future networks perform as more realistic training data becomes available.</strong></p> <d-cite key="7410673"></d-cite> <hr/>]]></content><author><name>Ronak Haresh Chhatbar</name></author><category term="Deep-Learning"/><category term="Computer-Vision"/><category term="AIResearch"/><category term="OpticalFlow"/><category term="Robotics"/><summary type="html"><![CDATA[Exploring the Innovations in Flownet paper approach for optical flow estimation.]]></summary></entry><entry><title type="html">ImageNet Classification with Deep Convolutional Neural Networks</title><link href="https://alphapibeta.github.io/blog/2024/imagenet/" rel="alternate" type="text/html" title="ImageNet Classification with Deep Convolutional Neural Networks"/><published>2024-01-18T00:00:00+00:00</published><updated>2024-01-18T00:00:00+00:00</updated><id>https://alphapibeta.github.io/blog/2024/imagenet</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2024/imagenet/"><![CDATA[<h2 id="1-introduction">1. Introduction</h2> <blockquote> <p><strong>Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse.</strong></p> </blockquote> <ol> <li><strong>Stationarity of Statistics</strong>: Images’ statistical properties, such as texture and color distributions, are assumed to be consistent across different regions of the image. This enables CNNs to apply the same filters to the entire image, significantly reducing the model’s complexity.</li> <li><strong>Locality of Pixel Dependencies</strong>: CNNs operate under the assumption that nearby pixels are more likely to be connected. In order to enable the network to recognize local features like edges and shapes, this is accomplished by applying tiny, localized filters (kernels) to the image.</li> </ol> <p>Mathematically, a convolution operation in a CNN can be represented as:</p> \[f(x, y) = \sum_{i=-a}^{a} \sum_{j=-b}^{b} K(i, j) \cdot I(x-i, y-j)\] <p>Here, ( f(x, y) ) is the output after applying the convolution operation to the input image ( I ) at position ( (x, y) ). The kernel ( K(i, j) ) is applied over the image such that each element of the kernel is multiplied by the corresponding element of the image under it, and the results are summed to produce the output ( f(x, y) ). The kernel ( K ) can be expressed as a matrix:</p> \[K = \begin{bmatrix} k_{11} &amp; k_{12} &amp; k_{13} \\ k_{21} &amp; k_{22} &amp; k_{23} \\ k_{31} &amp; k_{32} &amp; k_{33} \end{bmatrix}\] <p>With fewer parameters than fully connected networks, CNNs can learn hierarchical representations of images thanks to these principles, which makes them particularly useful for tasks like object detection and image classification.</p> <h2 id="2-dataset">2. Dataset</h2> <p>ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories.</p> <h2 id="3-the-architecture">3. The Architecture</h2> <blockquote> <p><strong>3.1 ReLU Nonlinearity</strong></p> <blockquote> <p><strong>In terms of training time with gradient descent, these saturating nonlinearities are much slower than the non-saturating nonlinearity f(x) = max(0,x). Following Nair and Hinton [20], we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs). Deep convolutional neural net- works with ReLUs train several times faster than their equivalents with tanh units.</strong></p> </blockquote> </blockquote> <p>The activation function is a core component in neural networks that introduces non-linearity, affecting the network’s ability to learn complex patterns. Traditional activation functions like the hyperbolic tangent (<code class="language-plaintext highlighter-rouge">tanh</code>) or the sigmoid function can cause the gradient to vanish during backpropagation, which slows down the training.</p> <p>The Rectified Linear Unit (ReLU) has become the de facto standard in neural network activation due to its non-saturating form, defined as:</p> \[ReLU(x) = \max(0, x)\] <p>This simple yet powerful function maintains the gradient, preventing the vanishing gradient problem and allowing the network to learn faster. Neurons with ReLU activation are only activated when the input is positive, leading to sparse activation within the network.</p> <p>CNNs with ReLUs train substantially faster than those with <code class="language-plaintext highlighter-rouge">tanh</code> units. This efficiency gain is depicted in the figure below, which demonstrates how a four-layer convolutional network with ReLUs can reach a 25% training error rate on CIFAR-10 much quicker than the same network utilizing <code class="language-plaintext highlighter-rouge">tanh</code> neurons.</p> <div class="row mt-10"> <div class="col-md-6 col-sm-2 mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/imagenet_relu-480.webp 480w,/assets/img/imagenet_relu-800.webp 800w,/assets/img/imagenet_relu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/imagenet_relu.png" class="img-fluid rounded z-depth-4 img-smaller" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 1: A comparison of training times between a CNN with ReLU activation and one with tanh activation.</div> </div> </div> <p>Empirical results show that networks with ReLU not only train faster but also achieve better performance on complex tasks, which is particularly beneficial when dealing with large datasets and deep architectures.</p> <blockquote> <p><strong>3.3 Local Response Normalization</strong></p> <blockquote> <p><strong>ReLUs have the desirable property that they do not require input normalization to prevent them from saturating. If at least some training examples produce a positive input to a ReLU, learning will happen in that neuron. However, we still find that the following local normalization scheme aids generalization.</strong></p> </blockquote> </blockquote> <h3 id="the-role-in-relu-networks">The Role in ReLU Networks</h3> <p>While Rectified Linear Units (ReLUs) have the advantage of not requiring input normalization to prevent saturation, we discovered an additional normalization technique that enhances generalization. This is known as Local Response Normalization (LRN).</p> <h3 id="mechanism-of-lrn">Mechanism of LRN</h3> <p>Given the activity \(a*{i}^{x,y}\) of a neuron, calculated by applying \(kernel ( i )\) at position \((x, y)\) and then passing it through a ReLU nonlinearity, the response-normalized activity \(( b*{i}^{x,y} )\) can be described by:</p> \[b_{i}^{x,y} = \frac{a_{i}^{x,y}}{\left(k + \alpha \sum_{j=\max(0, i-n/2)}^{\min(N-1, i+n/2)} (a_{j}^{x,y})^2\right)^\beta}\] <p>Here, the sum runs over ( n ) “adjacent” kernel maps at the same spatial position, and ( N ) represents the total number of kernels in the layer. This normalization is inspired by lateral inhibition found in biological neurons, inducing competition among neuron outputs computed with different kernels.</p> <blockquote> <p><strong>3.4 Overlapping Pooling</strong></p> </blockquote> <p>If a pooling layer has a grid of pooling units, they are spaced <code class="language-plaintext highlighter-rouge">s</code> pixels apart and each summarizes a <code class="language-plaintext highlighter-rouge">z × z</code> neighborhood. Traditional pooling sets <code class="language-plaintext highlighter-rouge">s = z</code>, meaning no overlap between the neighborhoods.</p> <p><strong>However</strong>, Imagenet employs an overlapping pooling strategy. They set <code class="language-plaintext highlighter-rouge">s &lt; z</code>. Specifically, we use <code class="language-plaintext highlighter-rouge">s = 2</code> and <code class="language-plaintext highlighter-rouge">z = 3</code>. <strong>This overlapping approach has been found to be more effective in reducing error rates</strong>.</p> <blockquote> <p><strong>3.5 Overall Architecture</strong></p> <blockquote> <p><strong>As depicted in Figure 2, the net contains eight layers with weights; the first five are convolutional and the remaining three are fully- connected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution.</strong></p> </blockquote> </blockquote> <p>The approximately 60 million trainable parameters of Imagenet are a crucial component. This large number of parameters improves the network’s classification performance by enabling it to learn intricate and subtle features from the vast and diverse ImageNet dataset. Throughout the training phase, these parameters are changed to allow the network to perform as well as possible on the dataset.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/imagenet-480.webp 480w,/assets/img/imagenet-800.webp 800w,/assets/img/imagenet-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/imagenet.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 2: Architecture of a Convolutional Neural Network.</div> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Imagenet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) // 23,232 parameters
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) // 307,200 parameters
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) // 663,552 parameters
    (7): ReLU(inplace=True)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) // 884,736 parameters
    (9): ReLU(inplace=True)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) // 589,824 parameters
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True) // 37,748,736 parameters
    (2): ReLU(inplace=True)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=4096, bias=True) // 16,777,216 parameters
    (5): ReLU(inplace=True)
    (6): Linear(in_features=4096, out_features=1000, bias=True) // 4,096,000 parameters
  )
)
</code></pre></div></div> <blockquote> <p><strong>4 Reducing Overfitting</strong></p> <p><strong>Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC make each training example impose 10 bits of constraint on the mapping from image to label, this turns out to be insufficient to learn so many parameters without considerable overfitting. Below, we describe the two primary ways in which we combat overfitting.</strong></p> </blockquote> <blockquote> <p><strong>4.1 Data Augmentation</strong></p> <blockquote> <p>We employ two distinct forms of data augmentation</p> </blockquote> </blockquote> <ul> <li>Generating image translations and horizontal reflections</li> <li>PCA Color Augmentation</li> </ul> <h2 id="pca-color-augmentation">PCA Color Augmentation</h2> <p>PCA Color Augmentation is a technique to augment image data by adjusting the intensities of the RGB channels. This method helps the model become more invariant to changes in illumination, which is a common property of natural images.</p> <p>The process involves the following steps:</p> <ol> <li>Perform PCA on the RGB pixel values across the entire ImageNet training set to obtain principal components.</li> <li>For each training image, alter the pixel values by adding a certain amount of these principal components.</li> </ol> <p>Mathematically, for each pixel \(I\_{xy} = [I_R, I_G, I_B]^T\) in the image, we add the following quantity:</p> \[[p_1, p_2, p_3][\alpha_1 \lambda_1, \alpha_2 \lambda_2, \alpha_3 \lambda_3]^T\] <p>Here:</p> <ul> <li>\(p_i\) represents the \(i\)-th eigenvector from the PCA of the RGB pixel values.</li> <li>\(\lambda_i\) is the corresponding eigenvalue, which indicates the variance in the direction of the eigenvector.</li> <li>\(\alpha_i\) is a random variable drawn from a Gaussian distribution with mean zero and standard deviation 0.1. It is sampled once per image and used across all pixels, ensuring consistency within the image.</li> </ul> <p>By scaling the principal components by their corresponding eigenvalues and the random variable, we effectively simulate realistic variations in lighting. This adjustment to the pixel values does not alter the content or structure of the image, preserving the label of the image while providing a form of regularization.</p> <blockquote> <p><strong>4.2 Dropout</strong></p> <blockquote> <p><strong>The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in back- propagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.</strong></p> </blockquote> </blockquote> <p>To stop overfitting in neural networks, dropout is a regularization technique used. The network is forced to learn robust features that are independent of any particular set of neurons by randomly setting a portion of the neuron outputs to zero during training. Overfitting is essentially reduced by the randomness in neuron activation, which improves the model’s ability to generalize to new data. Neuron outputs are adjusted to compensate for the deactivated neurons during training, but dropout is not applied during testing.</p> <h2 id="results">Results</h2> <h2 id="comparison-of-model-performances">Comparison of Model Performances</h2> <p>The paper presents a comparison of the convolutional neural network (CNN) model with other notable methods. Below is a summary of their performances on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) for the years 2010 and 2012:</p> <table> <thead> <tr> <th>Model</th> <th>Top-1 Error (ILSVRC-2010)</th> <th>Top-5 Error (ILSVRC-2010)</th> <th>Top-5 Error (ILSVRC-2012)</th> </tr> </thead> <tbody> <tr> <td>CNN</td> <td>37.5%</td> <td>17.0%</td> <td>15.3%</td> </tr> <tr> <td>Sparse Coding [2]</td> <td>47.1%</td> <td>28.2%</td> <td>26.2% (val)</td> </tr> <tr> <td>SIFT + FVs [24]</td> <td>45.7%</td> <td>25.7%</td> <td>-</td> </tr> </tbody> </table> <h3 id="analysis">Analysis</h3> <ul> <li><strong>CNN</strong>: Demonstrates a notable decrease in the top-1 and top-5 error rates. This demonstrates how deep convolutional neural networks work well for tasks involving image classification.</li> <li><strong>Sparse Coding [2]</strong>: Even though it works well, this approach has higher error rates than CNN, which highlights how much better deep learning techniques perform.</li> <li><strong>SIFT + FVs [24]</strong>: It shows higher error rates, just like Sparse Coding. The comparison demonstrates how CNNs have improved at handling challenging image recognition tasks.</li> </ul> <p>This comparative analysis illustrates the groundbreaking impact of CNNs in reducing error rates and setting new benchmarks in image classification.</p> <h2 id="conclusion">Conclusion</h2> <p>By utilizing the expansive ImageNet dataset, the network achieved remarkable accuracy, showcasing a significant leap over previous state-of-the-art results. Key innovations include the use of ReLU nonlinearity, dropout for overfitting reduction, and extensive data augmentation, contributing to the network’s robust performance.</p> <hr/> <h3 id="citation">Citation</h3> <p>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. NIPS.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="Deep-Learning,"/><category term="Computer-Vision,"/><category term="AIResearch"/><summary type="html"><![CDATA[Understanding Novalties of Imagenet classification]]></summary></entry><entry><title type="html">Efficient Estimation of Word Representations in Vector Space-2013</title><link href="https://alphapibeta.github.io/blog/2024/Word2Vec/" rel="alternate" type="text/html" title="Efficient Estimation of Word Representations in Vector Space-2013"/><published>2024-01-18T00:00:00+00:00</published><updated>2024-01-18T00:00:00+00:00</updated><id>https://alphapibeta.github.io/blog/2024/Word2Vec</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2024/Word2Vec/"><![CDATA[<h2 id="1-abstract">1. Abstract</h2> <blockquote> <p><strong>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities..</strong></p> </blockquote> <p>This paper introduces innovative model architectures that diverge from traditional N-gram models in statistical language modeling, offering a fresh approach to comprehend relationships within a vocabulary. These models prioritize computational efficiency while maintaining high accuracy in representing word semantics and syntactic relationships, showcasing a significant advancement in natural language processing.</p> <h2 id="11-goals-of-the-paper">1.1 Goals of the Paper</h2> <blockquote> <p><strong>The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary.We use recently proposed techniques for measuring the quality of the resulting vector representa- tions, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity.</strong></p> </blockquote> <h2 id="2-model-architectures">2. Model Architectures</h2> <blockquote> <p><strong>In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.</strong></p> </blockquote> <p>The training complexity for the models is given by:</p> \[O = E \times T \times Q\] <p>where ( O ) represents the overall training complexity, ( E ) is the number of training epochs, ( T ) denotes the number of words in the training set, and ( Q ) is a model-specific factor.</p> <p>Models are trained using stochastic gradient descent and backpropagation.</p> <p><br/></p> <blockquote> <p><strong>2.1 Feedforward Neural Net Language Model (NNLM)</strong></p> <blockquote> <p>The Feedforward Neural Network Language Model (NNLM) is structured with several layers: input, projection, hidden, and output.</p> </blockquote> </blockquote> <h3 id="input-layer">Input Layer</h3> <p>At the input layer, the model considers ( N ) previous words. These words are encoded using 1-of-( V ) coding, where ( V ) represents the size of the vocabulary.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
</code></pre></div></div> <h3 id="projection-layer">Projection Layer</h3> <p>The encoded input is then mapped to a projection layer ( P ) with dimensionality ( N \times D ), using a shared projection matrix. This step is computationally efficient since only ( N ) inputs are active simultaneously.</p> <h3 id="hidden-layer">Hidden Layer</h3> <p>The architecture’s complexity increases between the projection and hidden layers due to the dense nature of projection layer values. For a typical choice of ( N = 10 ), the projection layer size (( P )) ranges from 500 to 2000, while the hidden layer (( H )) usually comprises 500 to 1000 units.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">context_size</span> <span class="o">*</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
</code></pre></div></div> <h3 id="output-layer">Output Layer</h3> <p>The hidden layer is essential for computing the probability distribution over the vocabulary words, resulting in an output layer with dimensionality ( V ). The computational complexity per training example can be expressed as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
</code></pre></div></div> \[Q = N \times D + N \times D \times H + H \times V\] <p>In this equation, ( H \times V ) is the dominant term. However, to mitigate this complexity, several practical solutions have been proposed. These include using hierarchical versions of the softmax or employing models that</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">context_size</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># NNLM Model Structure
</span><span class="nc">NNLM</span><span class="p">(</span>
  <span class="p">(</span><span class="n">embeddings</span><span class="p">):</span> <span class="nc">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="o">//</span> <span class="mi">100</span><span class="p">,</span><span class="mi">000</span> <span class="n">parameters</span>
  <span class="p">(</span><span class="n">linear1</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">//</span> <span class="mi">38</span><span class="p">,</span><span class="mi">528</span> <span class="n">parameters</span>
  <span class="p">(</span><span class="n">linear2</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">//</span> <span class="mi">129</span><span class="p">,</span><span class="mi">000</span> <span class="n">parameters</span>
<span class="p">)</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">context_size</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># NNLM Model Structure with Larger Vocabulary
</span><span class="nc">NNLM</span><span class="p">(</span>
  <span class="p">(</span><span class="n">embeddings</span><span class="p">):</span> <span class="nc">Embedding</span><span class="p">(</span><span class="mi">1000000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="o">//</span> <span class="mi">100</span><span class="p">,</span><span class="mi">000</span><span class="p">,</span><span class="mi">000</span> <span class="n">parameters</span>
  <span class="p">(</span><span class="n">linear1</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">//</span> <span class="mi">38</span><span class="p">,</span><span class="mi">528</span> <span class="n">parameters</span>
  <span class="p">(</span><span class="n">linear2</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">//</span> <span class="mi">128</span><span class="p">,</span><span class="mi">999</span><span class="p">,</span><span class="mi">872</span> <span class="n">parameters</span>
<span class="p">)</span>
</code></pre></div></div> <h3 id="hierarchical-softmax">Hierarchical Softmax</h3> <p>The model utilize hierarchical softmax, where the vocabulary is represented as a Huffman binary tree. This approach capitalizes on the observation that word frequency effectively determines classes in neural net language models. Huffman trees assign shorter binary codes to more frequent words, reducing the number of output units needing evaluation. While a balanced binary tree would require evaluating \(( \log_2(V) )\) outputs, a Huffman tree-based hierarchical softmax needs only about \(( \log_2( ext{Unigram perplexity}(V)) )\). This results in a significant speedup, especially beneficial when the vocabulary size is large, such as one million words.</p> <blockquote> <p><strong>2.2 Recurrent Neural Net Language Model (RNNLM)</strong></p> <blockquote> <p><strong>Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N ), and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and output layer. What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections. This allows the recurrent model to form some kind of short term memory, as information from the past can be represented by the hidden layer state that gets updated based on the current input and the state of the hidden layer in the previous time step.</strong></p> </blockquote> </blockquote> <p>The complexity per training example of the RNN model is given by:</p> \[Q = H \times H + H \times V\] <p>where the word representations \(( D )\) have the same dimensionality as the hidden layer \(( H )\). Again, the term \(( H \times V )\) can be efficiently reduced to \(( H \times \log_2(V) )\) by using hierarchical softmax. Most of the complexity then comes from \(( H \times H )\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">RNNLM</span><span class="p">(</span>
  <span class="p">(</span><span class="n">rnn</span><span class="p">):</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span> <span class="o">//</span> <span class="mi">1296640</span> <span class="n">parameters</span>
  <span class="p">(</span><span class="n">linear</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">//</span> <span class="mi">1290000</span> <span class="n">parameters</span>
<span class="p">)</span>
</code></pre></div></div> <h2 id="3-new-log-linear-models">3. New Log-linear Models</h2> <blockquote> <p><strong>In this section, we propose two new model architectures for learning distributed representations of words that try to minimize computational complexity. The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model. While this is what makes neural networks so attractive, we decided to explore simpler models that might not be able to represent the data as precisely as neural networks, but can possibly be trained on much more data efficiently.</strong></p> <blockquote> <p><strong>3.1 Continuous Bag-of-Words Model</strong></p> <blockquote> <p><strong>The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged). We call this archi- tecture a bag-of-words model as the order of words in the history does not influence the projection. Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word.</strong></p> </blockquote> </blockquote> </blockquote> <p>The training complexity of the CBOW model is captured by the formula:</p> \[Q = N \times D + D \times \log_2(V)\] <p>Here, ( Q ) is the training complexity, ( N ) is the number of context words (both past and future), ( D ) is the size of the word embeddings, and ( V ) is the vocabulary size. This model is denoted as CBOW, distinguishing it from traditional bag-of-words models by its use of continuous distributed representations for context.</p> <h5 id="note-that-the-weight-matrix-between-the-input-and-the-projection-layer-is-shared-for-all-word-positions-in-the-same-way-as-in-the-nnlm"><strong>Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.</strong></h5> <blockquote> <blockquote> <p><strong>3.2 Continuous Skip-gram Model</strong></p> <blockquote> <p><strong>The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. More precisely, we use each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word. We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples.</strong></p> </blockquote> </blockquote> </blockquote> <p>The training complexity for the Skip-gram model is given by:</p> \[Q = C \times (D + D \times \log_2(V))\] <p>Here, ( Q ) denotes the training complexity, ( C ) represents the context window size, ( D ) is the dimensionality of the word embeddings, and ( V ) is the vocabulary size. The model effectively performs classifications for each word, using both preceding and following words within the context window as positive examples, with distant words weighted less significantly.</p> <div class="row mt-10"> <div class="col-md-12 col-sm-12 mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/word2vec-480.webp 480w,/assets/img/word2vec-800.webp 800w,/assets/img/word2vec-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/word2vec.png" class="img-fluid rounded z-depth-4" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 1: New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.</div> </div> </div> <h2 id="4-results-and-observations">4. Results and Observations</h2> <blockquote> <p><strong>We found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented..</strong></p> </blockquote> <p>Through algebraic operations on word vectors, we can uncover relationships and analogies, bringing a new dimension to understanding language semantics. The method is simple: subtract the vector representing one concept from another, then add a third, and search for the nearest word vector to this result. For instance, to find a word related to ‘small’ in the same way ‘biggest’ is related to ‘big’, we compute:</p> \[\text{vector(smallest)} \approx \text{vector(biggest)} - \text{vector(big)} + \text{vector(small)}\] <p>This computation, when applied to well-trained word vectors, often yields the correct association.</p> <h2 id="5-comparative-analysis-of-model-architectures">5. Comparative Analysis of Model Architectures</h2> <blockquote> <p><strong>A comparative study of various word vector models shows that simpler architectures like CBOW and Skip-gram can surpass more complex neural networks in both syntactic and semantic tasks, underscoring the potential of log-linear models in processing vast datasets.</strong></p> </blockquote> <p>Comparative performance of word vector models on semantic and syntactic accuracy:</p> <table> <thead> <tr> <th>Model Architecture</th> <th>Semantic Accuracy (%)</th> <th>Syntactic Accuracy (%)</th> </tr> </thead> <tbody> <tr> <td>RNNLM</td> <td>9</td> <td>36</td> </tr> <tr> <td>NNLM</td> <td>23</td> <td>53</td> </tr> <tr> <td>CBOW</td> <td>24</td> <td>64</td> </tr> <tr> <td>Skip-gram</td> <td>55</td> <td>59</td> </tr> </tbody> </table> <p><br/></p> <h5 id="the-skip-gram-model-in-particular-shines-in-its-ability-to-understand-semantic-relationships-outperforming-other-models-this-is-a-testament-to-the-potential-of-these-simpler-models-in-handling-large-scale-data-effectively">The Skip-gram model, in particular, shines in its ability to understand semantic relationships, outperforming other models. This is a testament to the potential of these simpler models in handling large-scale data effectively</h5> <p><br/></p> <h2 id="conclusion">Conclusion</h2> <blockquote> <p><strong>In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set..</strong></p> </blockquote> <h3 id="citation">Citation</h3> <p>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. arXiv preprint arXiv:1301.3781.</p> <p><em>Date: January 21, 2024</em></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="Deep-Learning,"/><category term="NLP,"/><category term="AIResearch"/><summary type="html"><![CDATA[Understanding Novalties of Word Representations in Vector Space (Word2Vec).]]></summary></entry><entry><title type="html">Introduction to Kernel density estimation.</title><link href="https://alphapibeta.github.io/blog/2019/introduction-to-kernel-density-estimation/" rel="alternate" type="text/html" title="Introduction to Kernel density estimation."/><published>2019-08-31T07:37:27+00:00</published><updated>2019-08-31T07:37:27+00:00</updated><id>https://alphapibeta.github.io/blog/2019/introduction-to-kernel-density-estimation</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2019/introduction-to-kernel-density-estimation/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Box-Cox Transformation.</title><link href="https://alphapibeta.github.io/blog/2019/box-cox-transformation/" rel="alternate" type="text/html" title="Box-Cox Transformation."/><published>2019-08-25T11:04:02+00:00</published><updated>2019-08-25T11:04:02+00:00</updated><id>https://alphapibeta.github.io/blog/2019/box-cox-transformation</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2019/box-cox-transformation/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Autoencoders — Escape the curse of dimensionality.</title><link href="https://alphapibeta.github.io/blog/2019/autoencodersescape-the-curse-of-dimensionality/" rel="alternate" type="text/html" title="Autoencoders — Escape the curse of dimensionality."/><published>2019-04-01T07:36:30+00:00</published><updated>2019-04-01T07:36:30+00:00</updated><id>https://alphapibeta.github.io/blog/2019/autoencodersescape-the-curse-of-dimensionality</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2019/autoencodersescape-the-curse-of-dimensionality/"><![CDATA[]]></content><author><name></name></author></entry></feed>