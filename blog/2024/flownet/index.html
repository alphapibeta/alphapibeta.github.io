<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> FlowNet Learning Optical Flow with Convolutional Networks | Ronak Haresh Chhatbar </title> <meta name="author" content="Ronak Haresh Chhatbar"> <meta name="description" content="Exploring the Innovations in Flownet paper approach for optical flow estimation."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alphapibeta.github.io/blog/2024/flownet/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "FlowNet Learning Optical Flow with Convolutional Networks",
            "description": "Exploring the Innovations in Flownet paper approach for optical flow estimation.",
            "published": "January 30, 2024",
            "authors": [
              
              {
                "author": "Ronak Haresh Chhatbar",
                "authorURL": "https://alphapibeta.github.io/",
                "affiliations": [
                  {
                    "name": "Spatial AI and Robotics lab",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ronak </span> Haresh Chhatbar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>FlowNet Learning Optical Flow with Convolutional Networks</h1> <p>Exploring the Innovations in Flownet paper approach for optical flow estimation.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#abstract">Abstract</a> </div> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#network-architectures">Network Architectures</a> </div> <div> <a href="#contracting-part">Contracting part</a> </div> <div> <a href="#correlation-layer-in-flownet">Correlation Layer in FlowNet</a> </div> <div> <a href="#expanding-part">Expanding part</a> </div> <div> <a href="#datasets">Datasets</a> </div> <div> <a href="#results">Results</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="abstract">Abstract</h2> <p><strong>Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.</strong></p> <h2 id="introduction">Introduction</h2> <blockquote> <p><strong>optical flow estimation needs precise per-pixel localization, it also requires finding correspondences between two input images. This involves not only learning image feature representations, but also learning to match them at different locations in the two images. In this respect, optical flow estimation fundamentally differs from previous applications of CNNs.</strong></p> </blockquote> <div class="row mt-10"> <div class="col-md-10 col-sm-10 mt-10 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flownet01-480.webp 480w,/assets/img/flownet01-800.webp 800w,/assets/img/flownet01-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/flownet01.png" class="img-fluid rounded z-depth-4 img-smaller" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption">Figure 1:Efficacy of Correlation Layer in CNNs for Optical Flow Prediction.</div> </div> </div> <p>The network intorduces a new layer called <code class="language-plaintext highlighter-rouge">correlation layer</code> that significantly improves optical flow estimation by efficiently finding correspondences between different image patches.</p> <h2 id="network-architectures">Network Architectures</h2> <p>This paper takes an end-to-end learning approach to predicting optical flow: given a dataset consisting of image pairs and ground truth flows, we train a network to predict the x–y flow fields directly from the images.</p> <p><code class="language-plaintext highlighter-rouge">Pooling in CNNs is necessary to make network training computationally feasible and, more fundamentally, to allow aggregation of information over large areas of the input images</code></p> <p>Networks consisting of <code class="language-plaintext highlighter-rouge">contracting</code> and <code class="language-plaintext highlighter-rouge">expanding</code> parts are trained as a whole using backpropagation. Architectures we use are depicted in Figures 3 and 4.</p> <h2 id="contracting-part"><strong>Contracting part</strong></h2> <ul> <li> <p>A simple choice is to stack both input images together and feed them through a rather generic network, allowing the network to decide itself how to process Figure 2. Refinement of the coarse feature maps to the high resothe image pair to extract the motion information. This is illution prediction. lustrated in Figure 3.</p> </li> <li> <p>Another approach is to create two separate, yet identical processing streams for the two images and to combine them at a later stage as shown in Fig. 3. With this architecture the network is constrained to first produce meaningful representations of the two images separately and then combine them on a higher level</p> </li> </ul> <h3 id="correlation-layer-in-flownet">Correlation Layer in FlowNet</h3> <p>The correlation layer is a pivotal component of FlowNet’s architecture, allowing the network to compare patches from feature maps \(( f_1 )\) and \(( f_2 )\) derived from two input images. This comparison is fundamental for optical flow estimation, as it involves finding correspondences between different locations in these images.</p> <h4 id="mathematical-formulation">Mathematical Formulation</h4> <p>The correlation of two patches centered at \(( x_1 )\)in the first map and \(( x_2 )\) in the second map is defined as:</p> \[c(x_1, x_2) = \sum_{o \in [-k, k] \times [-k, k]} \langle f_1(x_1 + o), f_2(x_2 + o) \rangle\] <p>where \(( \langle \cdot, \cdot \rangle )\) denotes the scalar product, and the summation is over a square patch of size \(( K := 2k + 1 )\). This equation is similar to a convolution operation but involves convolving data with other data, not with a filter.</p> <h4 id="computational-considerations">Computational Considerations</h4> <p>For each location \(( x_1 )\), the correlations \(( c(x_1, x_2) )\) are computed only in a neighborhood of size \(( D := 2d + 1 )\), where \(( d )\) is the maximum displacement allowed. This is to make the computation tractable, as comparing all possible patch combinations without this limitation would be computationally expensive.</p> <p>The computational cost of computing \(( c(x_1, x_2) )\) involves \(( c \cdot K^2 )\) multiplications, and the total cost for all patch combinations is proportional to \(( w^2 \cdot h^2 )\), where \(( w )\) and \(( h )\) are the width and height of the feature maps.</p> <h4 id="implementation">Implementation</h4> <p>In practice, the output of the correlation layer is organized into channels, representing relative displacements. This results in an output size of \(( (w \times h \times D^2) )\). For the backward pass, derivatives are implemented with respect to each input feature map.</p> <p>estimates the computational complexity for computing feature map correlations, factoring in patch size (K), dimensions (w and h), and channel count (c) of the feature maps.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flownet02-480.webp 480w,/assets/img/flownet02-800.webp 800w,/assets/img/flownet02-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/flownet02.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption">Figure 2: Flownet simple.</div> </div> </div> <p><br></p> <p><code class="language-plaintext highlighter-rouge">If we notice the architecture we see that the images are stacked together channel wise.</code></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flownet03-480.webp 480w,/assets/img/flownet03-800.webp 800w,/assets/img/flownet03-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/flownet03.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption">Figure 3: Flownet Corelation.</div> </div> </div> <h2 id="expanding-part"><strong>Expanding part</strong></h2> <p>The main ingredient of the expanding part are <code class="language-plaintext highlighter-rouge">upconvolutional layers</code>, consisting of unpooling (extending the feature maps, as opposed to pooling) and a convolution. Such layers have been used previously [38, 37, 16, 28, 9]. To perform the refinement, we apply the ‘upconvolution’ to feature maps, and concatenate it with corresponding feature maps from the ’contractive’ part of the network and an upsampled coarser flow prediction (if available). This way we preserve both the high-level information passed from coarser feature maps and fine local information provided in lower layer feature maps. Each step increases the resolution twice. We repeat this 4 times, resulting in a predicted flow for which the resolution is still 4 times smaller than the input.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flownet04-480.webp 480w,/assets/img/flownet04-800.webp 800w,/assets/img/flownet04-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/flownet04.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption">Figure 4: refinement-layrer.</div> </div> </div> <h2 id="datasets">Datasets</h2> <p><strong>Flying Chairs The Sintel dataset is still too small to train large CNNs. To provide enough training data, we create a simple syn- thetic dataset, which we name Flying Chairs, by applying affine transformations to images collected from Flickr and a publicly available set of renderings of 3D chair models [1]. We retrieve 964 images from Flickr2 with a resolution of 1, 024 × 768 from the categories ‘city’ (321), ‘landscape’ (129) and ‘mountain’ (514). We cut the images into 4 quad- rants and use the resulting 512 × 384 image crops as back- ground. As foreground objects we add images of multi- ple chairs from [1] to the background. From the original dataset we remove very similar chairs, resulting in 809 chair types and 62 views per chair available. Examples are shown in Figure 5. To generate motion, we randomly sample 2D affine transformation parameters for the background and the chairs. The chairs’ transformations are relative to the back- ground transformation, which can be interpreted as both the camera and the objects moving. Using the transformation parameters we generate the second image, the ground truth optical flow and occlusion regions.</strong></p> <h2 id="results">Results</h2> <table> <thead> <tr> <th style="text-align: left">Method</th> <th style="text-align: right">Sintel Clean train</th> <th style="text-align: right">Sintel Clean test</th> <th style="text-align: right">Sintel Final train</th> <th style="text-align: right">Sintel Final test</th> <th style="text-align: right">KITTI train</th> <th style="text-align: right">KITTI test</th> <th style="text-align: right">Middlebury train</th> <th style="text-align: right">Middlebury test</th> <th style="text-align: right">Chairs test</th> <th style="text-align: right">Time (sec) CPU</th> <th style="text-align: right">Time (sec) GPU</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">EpicFlow [30]</td> <td style="text-align: right">2.27</td> <td style="text-align: right">4.12</td> <td style="text-align: right">3.57</td> <td style="text-align: right">6.29</td> <td style="text-align: right">3.47</td> <td style="text-align: right">3.8</td> <td style="text-align: right">0.31</td> <td style="text-align: right">3.55</td> <td style="text-align: right">2.94</td> <td style="text-align: right">16</td> <td style="text-align: right">-</td> </tr> <tr> <td style="text-align: left">DeepFlow [35]</td> <td style="text-align: right">3.19</td> <td style="text-align: right">5.38</td> <td style="text-align: right">4.40</td> <td style="text-align: right">7.21</td> <td style="text-align: right">4.58</td> <td style="text-align: right">5.8</td> <td style="text-align: right">0.21</td> <td style="text-align: right">4.22</td> <td style="text-align: right">3.53</td> <td style="text-align: right">17</td> <td style="text-align: right">-</td> </tr> <tr> <td style="text-align: left">EPPM [3]</td> <td style="text-align: right">-</td> <td style="text-align: right">6.49</td> <td style="text-align: right">-</td> <td style="text-align: right">8.38</td> <td style="text-align: right">-</td> <td style="text-align: right">9.2</td> <td style="text-align: right">-</td> <td style="text-align: right">3.36</td> <td style="text-align: right">-</td> <td style="text-align: right">-</td> <td style="text-align: right">0.2</td> </tr> <tr> <td style="text-align: left">LDOF [6]</td> <td style="text-align: right">4.19</td> <td style="text-align: right">7.56</td> <td style="text-align: right">6.28</td> <td style="text-align: right">9.12</td> <td style="text-align: right">13.73</td> <td style="text-align: right">12.4</td> <td style="text-align: right">0.45</td> <td style="text-align: right">4.55</td> <td style="text-align: right">3.47</td> <td style="text-align: right">65</td> <td style="text-align: right">2.5</td> </tr> <tr> <td style="text-align: left">FlowNetS</td> <td style="text-align: right">4.50</td> <td style="text-align: right">7.42</td> <td style="text-align: right">5.45</td> <td style="text-align: right">8.43</td> <td style="text-align: right">8.26</td> <td style="text-align: right">-</td> <td style="text-align: right">1.09</td> <td style="text-align: right">-</td> <td style="text-align: right">2.71</td> <td style="text-align: right">-</td> <td style="text-align: right">0.08</td> </tr> <tr> <td style="text-align: left">FlowNetS+v</td> <td style="text-align: right">3.66</td> <td style="text-align: right">6.45</td> <td style="text-align: right">4.76</td> <td style="text-align: right">7.67</td> <td style="text-align: right">6.50</td> <td style="text-align: right">-</td> <td style="text-align: right">0.33</td> <td style="text-align: right">-</td> <td style="text-align: right">2.86</td> <td style="text-align: right">-</td> <td style="text-align: right">1.05</td> </tr> <tr> <td style="text-align: left">FlowNetS+ft</td> <td style="text-align: right">6.96</td> <td style="text-align: right">7.76</td> <td style="text-align: right">7.76</td> <td style="text-align: right">7.22</td> <td style="text-align: right">7.52</td> <td style="text-align: right">9.1</td> <td style="text-align: right">15.20</td> <td style="text-align: right">-</td> <td style="text-align: right">3.04</td> <td style="text-align: right">-</td> <td style="text-align: right">0.08</td> </tr> <tr> <td style="text-align: left">FlowNetS+ft+v</td> <td style="text-align: right">6.16</td> <td style="text-align: right">7.22</td> <td style="text-align: right">7.22</td> <td style="text-align: right">7.22</td> <td style="text-align: right">6.07</td> <td style="text-align: right">7.6</td> <td style="text-align: right">3.84</td> <td style="text-align: right">0.47</td> <td style="text-align: right">4.58</td> <td style="text-align: right">-</td> <td style="text-align: right">1.05</td> </tr> <tr> <td style="text-align: left">FlowNetC</td> <td style="text-align: right">4.31</td> <td style="text-align: right">7.28</td> <td style="text-align: right">5.87</td> <td style="text-align: right">8.81</td> <td style="text-align: right">9.35</td> <td style="text-align: right">-</td> <td style="text-align: right">15.64</td> <td style="text-align: right">-</td> <td style="text-align: right">2.19</td> <td style="text-align: right">-</td> <td style="text-align: right">0.15</td> </tr> <tr> <td style="text-align: left">FlowNetC+v</td> <td style="text-align: right">3.57</td> <td style="text-align: right">6.27</td> <td style="text-align: right">5.25</td> <td style="text-align: right">8.01</td> <td style="text-align: right">7.45</td> <td style="text-align: right">-</td> <td style="text-align: right">3.92</td> <td style="text-align: right">-</td> <td style="text-align: right">2.61</td> <td style="text-align: right">-</td> <td style="text-align: right">1.12</td> </tr> <tr> <td style="text-align: left">FlowNetC+ft</td> <td style="text-align: right">6.85</td> <td style="text-align: right">8.51</td> <td style="text-align: right">8.51</td> <td style="text-align: right">7.88</td> <td style="text-align: right">7.31</td> <td style="text-align: right">-</td> <td style="text-align: right">12.33</td> <td style="text-align: right">-</td> <td style="text-align: right">2.27</td> <td style="text-align: right">-</td> <td style="text-align: right">0.15</td> </tr> <tr> <td style="text-align: left">FlowNetC+ft+v</td> <td style="text-align: right">6.08</td> <td style="text-align: right">7.88</td> <td style="text-align: right">7.88</td> <td style="text-align: right">7.88</td> <td style="text-align: right">6.07</td> <td style="text-align: right">7.6</td> <td style="text-align: right">3.81</td> <td style="text-align: right">0.50</td> <td style="text-align: right">4.52</td> <td style="text-align: right">-</td> <td style="text-align: right">1.12</td> </tr> </tbody> </table> <p>This Table represents Average endpoint errors (in pixels) of our networks compared to several well-performing methods on different datasets. The numbers in parentheses are the results of the networks on data they were trained on, and hence are not directly comparable to other results.</p> <ul> <li> <p><code class="language-plaintext highlighter-rouge">FlowNetS</code> generalizes better to datasets with additional complexities such as motion blur or fog, as evidenced by its performance on the Sintel Final dataset. In contrast, <code class="language-plaintext highlighter-rouge">FlowNetC</code> performs better on datasets with fewer complexities, such as Flying Chairs and Sintel Clean, indicating a proclivity to overfit to the type of data encountered during training. Despite having similar numbers of parameters, <code class="language-plaintext highlighter-rouge">FlowNetC's</code> overfitting may be advantageous with better training data.</p> </li> <li> <p>Furthermore, <code class="language-plaintext highlighter-rouge">FlowNetC</code> appears to struggle more with large displacements than <code class="language-plaintext highlighter-rouge">FlowNetS</code>, as demonstrated by its performance on the KITTI dataset and detailed analysis on Sintel Final. The constraint on maximum displacement within <code class="language-plaintext highlighter-rouge">FlowNetC's</code> correlation layer may be the limiting factor, which can be adjusted at the expense of computational efficiency.</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <p><strong>Building on recent progress in design of convolutional network architectures, we have shown that it is possible to train a network to directly predict optical flow from two in- put images. Intriguingly, the training data need not be re- alistic. The artificial Flying Chairs dataset including just affine motions of synthetic rigid objects is sufficient to pre- dict optical flow in natural scenes with competitive accu- racy. This proves the generalization capabilities of the pre- sented networks. On the test set of the Flying Chairs the CNNs even outperform state-of-the-art methods like Deep- Flow and EpicFlow. It will be interesting to see how future networks perform as more realistic training data becomes available.</strong></p> <d-cite key="7410673"></d-cite> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Ronak Haresh Chhatbar. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>