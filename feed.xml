<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://alphapibeta.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://alphapibeta.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-07T00:45:26+00:00</updated><id>https://alphapibeta.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">FlowNet Learning Optical Flow with Convolutional Networks</title><link href="https://alphapibeta.github.io/blog/2024/flownet/" rel="alternate" type="text/html" title="FlowNet Learning Optical Flow with Convolutional Networks"/><published>2024-01-30T00:00:00+00:00</published><updated>2024-01-30T00:00:00+00:00</updated><id>https://alphapibeta.github.io/blog/2024/flownet</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2024/flownet/"><![CDATA[<h2 id="abstract">Abstract</h2> <p><strong>Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.</strong></p> <h2 id="introduction">Introduction</h2> <blockquote> <p><strong>optical flow estimation needs precise per-pixel localization, it also requires finding correspondences between two input images. This involves not only learning image feature representations, but also learning to match them at different locations in the two images. In this respect, optical flow estimation fundamentally differs from previous applications of CNNs.</strong></p> </blockquote> <div class="row mt-10"> <div class="col-md-10 col-sm-10 mt-10 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flownet01-480.webp 480w,/assets/img/flownet01-800.webp 800w,/assets/img/flownet01-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/flownet01.png" class="img-fluid rounded z-depth-4 img-smaller" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 1:Efficacy of Correlation Layer in CNNs for Optical Flow Prediction.</div> </div> </div> <p>The network intorduces a new layer called <code class="language-plaintext highlighter-rouge">correlation layer</code> that significantly improves optical flow estimation by efficiently finding correspondences between different image patches.</p> <h2 id="network-architectures">Network Architectures</h2> <p>This paper takes an end-to-end learning approach to predicting optical flow: given a dataset consisting of image pairs and ground truth flows, we train a network to predict the x–y flow fields directly from the images.</p> <p><code class="language-plaintext highlighter-rouge">Pooling in CNNs is necessary to make network training computationally feasible and, more fundamentally, to allow aggregation of information over large areas of the input images</code></p> <p>Networks consisting of <code class="language-plaintext highlighter-rouge">contracting</code> and <code class="language-plaintext highlighter-rouge">expanding</code> parts are trained as a whole using backpropagation. Architectures we use are depicted in Figures 3 and 4.</p> <h2 id="contracting-part"><strong>Contracting part</strong></h2> <ul> <li> <p>A simple choice is to stack both input images together and feed them through a rather generic network, allowing the network to decide itself how to process Figure 2. Refinement of the coarse feature maps to the high resothe image pair to extract the motion information. This is illution prediction. lustrated in Figure 3.</p> </li> <li> <p>Another approach is to create two separate, yet identical processing streams for the two images and to combine them at a later stage as shown in Fig. 3. With this architecture the network is constrained to first produce meaningful representations of the two images separately and then combine them on a higher level</p> </li> </ul> <h3 id="correlation-layer-in-flownet">Correlation Layer in FlowNet</h3> <p>The correlation layer is a pivotal component of FlowNet’s architecture, allowing the network to compare patches from feature maps \(( f_1 )\) and \(( f_2 )\) derived from two input images. This comparison is fundamental for optical flow estimation, as it involves finding correspondences between different locations in these images.</p> <h4 id="mathematical-formulation">Mathematical Formulation</h4> <p>The correlation of two patches centered at \(( x_1 )\)in the first map and \(( x_2 )\) in the second map is defined as:</p> \[c(x_1, x_2) = \sum_{o \in [-k, k] \times [-k, k]} \langle f_1(x_1 + o), f_2(x_2 + o) \rangle\] <p>where \(( \langle \cdot, \cdot \rangle )\) denotes the scalar product, and the summation is over a square patch of size \(( K := 2k + 1 )\). This equation is similar to a convolution operation but involves convolving data with other data, not with a filter.</p> <h4 id="computational-considerations">Computational Considerations</h4> <p>For each location \(( x_1 )\), the correlations \(( c(x_1, x_2) )\) are computed only in a neighborhood of size \(( D := 2d + 1 )\), where \(( d )\) is the maximum displacement allowed. This is to make the computation tractable, as comparing all possible patch combinations without this limitation would be computationally expensive.</p> <p>The computational cost of computing \(( c(x_1, x_2) )\) involves \(( c \cdot K^2 )\) multiplications, and the total cost for all patch combinations is proportional to \(( w^2 \cdot h^2 )\), where \(( w )\) and \(( h )\) are the width and height of the feature maps.</p> <h4 id="implementation">Implementation</h4> <p>In practice, the output of the correlation layer is organized into channels, representing relative displacements. This results in an output size of \(( (w \times h \times D^2) )\). For the backward pass, derivatives are implemented with respect to each input feature map.</p> <p>estimates the computational complexity for computing feature map correlations, factoring in patch size (K), dimensions (w and h), and channel count (c) of the feature maps.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flownet02-480.webp 480w,/assets/img/flownet02-800.webp 800w,/assets/img/flownet02-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/flownet02.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 2: Flownet simple.</div> </div> </div> <p><br/></p> <p><code class="language-plaintext highlighter-rouge">If we notice the architecture we see that the images are stacked together channel wise.</code></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flownet03-480.webp 480w,/assets/img/flownet03-800.webp 800w,/assets/img/flownet03-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/flownet03.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 3: Flownet Corelation.</div> </div> </div> <h2 id="expanding-part"><strong>Expanding part</strong></h2> <p>The main ingredient of the expanding part are <code class="language-plaintext highlighter-rouge">upconvolutional layers</code>, consisting of unpooling (extending the feature maps, as opposed to pooling) and a convolution. Such layers have been used previously [38, 37, 16, 28, 9]. To perform the refinement, we apply the ‘upconvolution’ to feature maps, and concatenate it with corresponding feature maps from the ’contractive’ part of the network and an upsampled coarser flow prediction (if available). This way we preserve both the high-level information passed from coarser feature maps and fine local information provided in lower layer feature maps. Each step increases the resolution twice. We repeat this 4 times, resulting in a predicted flow for which the resolution is still 4 times smaller than the input.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flownet04-480.webp 480w,/assets/img/flownet04-800.webp 800w,/assets/img/flownet04-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/flownet04.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 4: refinement-layrer.</div> </div> </div> <h2 id="datasets">Datasets</h2> <p><strong>Flying Chairs The Sintel dataset is still too small to train large CNNs. To provide enough training data, we create a simple syn- thetic dataset, which we name Flying Chairs, by applying affine transformations to images collected from Flickr and a publicly available set of renderings of 3D chair models [1]. We retrieve 964 images from Flickr2 with a resolution of 1, 024 × 768 from the categories ‘city’ (321), ‘landscape’ (129) and ‘mountain’ (514). We cut the images into 4 quad- rants and use the resulting 512 × 384 image crops as back- ground. As foreground objects we add images of multi- ple chairs from [1] to the background. From the original dataset we remove very similar chairs, resulting in 809 chair types and 62 views per chair available. Examples are shown in Figure 5. To generate motion, we randomly sample 2D affine transformation parameters for the background and the chairs. The chairs’ transformations are relative to the back- ground transformation, which can be interpreted as both the camera and the objects moving. Using the transformation parameters we generate the second image, the ground truth optical flow and occlusion regions.</strong></p> <h2 id="results">Results</h2> <table> <thead> <tr> <th style="text-align: left">Method</th> <th style="text-align: right">Sintel Clean train</th> <th style="text-align: right">Sintel Clean test</th> <th style="text-align: right">Sintel Final train</th> <th style="text-align: right">Sintel Final test</th> <th style="text-align: right">KITTI train</th> <th style="text-align: right">KITTI test</th> <th style="text-align: right">Middlebury train</th> <th style="text-align: right">Middlebury test</th> <th style="text-align: right">Chairs test</th> <th style="text-align: right">Time (sec) CPU</th> <th style="text-align: right">Time (sec) GPU</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">EpicFlow [30]</td> <td style="text-align: right">2.27</td> <td style="text-align: right">4.12</td> <td style="text-align: right">3.57</td> <td style="text-align: right">6.29</td> <td style="text-align: right">3.47</td> <td style="text-align: right">3.8</td> <td style="text-align: right">0.31</td> <td style="text-align: right">3.55</td> <td style="text-align: right">2.94</td> <td style="text-align: right">16</td> <td style="text-align: right">-</td> </tr> <tr> <td style="text-align: left">DeepFlow [35]</td> <td style="text-align: right">3.19</td> <td style="text-align: right">5.38</td> <td style="text-align: right">4.40</td> <td style="text-align: right">7.21</td> <td style="text-align: right">4.58</td> <td style="text-align: right">5.8</td> <td style="text-align: right">0.21</td> <td style="text-align: right">4.22</td> <td style="text-align: right">3.53</td> <td style="text-align: right">17</td> <td style="text-align: right">-</td> </tr> <tr> <td style="text-align: left">EPPM [3]</td> <td style="text-align: right">-</td> <td style="text-align: right">6.49</td> <td style="text-align: right">-</td> <td style="text-align: right">8.38</td> <td style="text-align: right">-</td> <td style="text-align: right">9.2</td> <td style="text-align: right">-</td> <td style="text-align: right">3.36</td> <td style="text-align: right">-</td> <td style="text-align: right">-</td> <td style="text-align: right">0.2</td> </tr> <tr> <td style="text-align: left">LDOF [6]</td> <td style="text-align: right">4.19</td> <td style="text-align: right">7.56</td> <td style="text-align: right">6.28</td> <td style="text-align: right">9.12</td> <td style="text-align: right">13.73</td> <td style="text-align: right">12.4</td> <td style="text-align: right">0.45</td> <td style="text-align: right">4.55</td> <td style="text-align: right">3.47</td> <td style="text-align: right">65</td> <td style="text-align: right">2.5</td> </tr> <tr> <td style="text-align: left">FlowNetS</td> <td style="text-align: right">4.50</td> <td style="text-align: right">7.42</td> <td style="text-align: right">5.45</td> <td style="text-align: right">8.43</td> <td style="text-align: right">8.26</td> <td style="text-align: right">-</td> <td style="text-align: right">1.09</td> <td style="text-align: right">-</td> <td style="text-align: right">2.71</td> <td style="text-align: right">-</td> <td style="text-align: right">0.08</td> </tr> <tr> <td style="text-align: left">FlowNetS+v</td> <td style="text-align: right">3.66</td> <td style="text-align: right">6.45</td> <td style="text-align: right">4.76</td> <td style="text-align: right">7.67</td> <td style="text-align: right">6.50</td> <td style="text-align: right">-</td> <td style="text-align: right">0.33</td> <td style="text-align: right">-</td> <td style="text-align: right">2.86</td> <td style="text-align: right">-</td> <td style="text-align: right">1.05</td> </tr> <tr> <td style="text-align: left">FlowNetS+ft</td> <td style="text-align: right">6.96</td> <td style="text-align: right">7.76</td> <td style="text-align: right">7.76</td> <td style="text-align: right">7.22</td> <td style="text-align: right">7.52</td> <td style="text-align: right">9.1</td> <td style="text-align: right">15.20</td> <td style="text-align: right">-</td> <td style="text-align: right">3.04</td> <td style="text-align: right">-</td> <td style="text-align: right">0.08</td> </tr> <tr> <td style="text-align: left">FlowNetS+ft+v</td> <td style="text-align: right">6.16</td> <td style="text-align: right">7.22</td> <td style="text-align: right">7.22</td> <td style="text-align: right">7.22</td> <td style="text-align: right">6.07</td> <td style="text-align: right">7.6</td> <td style="text-align: right">3.84</td> <td style="text-align: right">0.47</td> <td style="text-align: right">4.58</td> <td style="text-align: right">-</td> <td style="text-align: right">1.05</td> </tr> <tr> <td style="text-align: left">FlowNetC</td> <td style="text-align: right">4.31</td> <td style="text-align: right">7.28</td> <td style="text-align: right">5.87</td> <td style="text-align: right">8.81</td> <td style="text-align: right">9.35</td> <td style="text-align: right">-</td> <td style="text-align: right">15.64</td> <td style="text-align: right">-</td> <td style="text-align: right">2.19</td> <td style="text-align: right">-</td> <td style="text-align: right">0.15</td> </tr> <tr> <td style="text-align: left">FlowNetC+v</td> <td style="text-align: right">3.57</td> <td style="text-align: right">6.27</td> <td style="text-align: right">5.25</td> <td style="text-align: right">8.01</td> <td style="text-align: right">7.45</td> <td style="text-align: right">-</td> <td style="text-align: right">3.92</td> <td style="text-align: right">-</td> <td style="text-align: right">2.61</td> <td style="text-align: right">-</td> <td style="text-align: right">1.12</td> </tr> <tr> <td style="text-align: left">FlowNetC+ft</td> <td style="text-align: right">6.85</td> <td style="text-align: right">8.51</td> <td style="text-align: right">8.51</td> <td style="text-align: right">7.88</td> <td style="text-align: right">7.31</td> <td style="text-align: right">-</td> <td style="text-align: right">12.33</td> <td style="text-align: right">-</td> <td style="text-align: right">2.27</td> <td style="text-align: right">-</td> <td style="text-align: right">0.15</td> </tr> <tr> <td style="text-align: left">FlowNetC+ft+v</td> <td style="text-align: right">6.08</td> <td style="text-align: right">7.88</td> <td style="text-align: right">7.88</td> <td style="text-align: right">7.88</td> <td style="text-align: right">6.07</td> <td style="text-align: right">7.6</td> <td style="text-align: right">3.81</td> <td style="text-align: right">0.50</td> <td style="text-align: right">4.52</td> <td style="text-align: right">-</td> <td style="text-align: right">1.12</td> </tr> </tbody> </table> <p>This Table represents Average endpoint errors (in pixels) of our networks compared to several well-performing methods on different datasets. The numbers in parentheses are the results of the networks on data they were trained on, and hence are not directly comparable to other results.</p> <ul> <li> <p><code class="language-plaintext highlighter-rouge">FlowNetS</code> generalizes better to datasets with additional complexities such as motion blur or fog, as evidenced by its performance on the Sintel Final dataset. In contrast, <code class="language-plaintext highlighter-rouge">FlowNetC</code> performs better on datasets with fewer complexities, such as Flying Chairs and Sintel Clean, indicating a proclivity to overfit to the type of data encountered during training. Despite having similar numbers of parameters, <code class="language-plaintext highlighter-rouge">FlowNetC's</code> overfitting may be advantageous with better training data.</p> </li> <li> <p>Furthermore, <code class="language-plaintext highlighter-rouge">FlowNetC</code> appears to struggle more with large displacements than <code class="language-plaintext highlighter-rouge">FlowNetS</code>, as demonstrated by its performance on the KITTI dataset and detailed analysis on Sintel Final. The constraint on maximum displacement within <code class="language-plaintext highlighter-rouge">FlowNetC's</code> correlation layer may be the limiting factor, which can be adjusted at the expense of computational efficiency.</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <p><strong>Building on recent progress in design of convolutional network architectures, we have shown that it is possible to train a network to directly predict optical flow from two in- put images. Intriguingly, the training data need not be re- alistic. The artificial Flying Chairs dataset including just affine motions of synthetic rigid objects is sufficient to pre- dict optical flow in natural scenes with competitive accu- racy. This proves the generalization capabilities of the pre- sented networks. On the test set of the Flying Chairs the CNNs even outperform state-of-the-art methods like Deep- Flow and EpicFlow. It will be interesting to see how future networks perform as more realistic training data becomes available.</strong></p> <d-cite key="7410673"></d-cite> <hr/>]]></content><author><name>Ronak Haresh Chhatbar</name></author><category term="Deep-Learning"/><category term="Computer-Vision"/><category term="AIResearch"/><category term="OpticalFlow"/><category term="Robotics"/><summary type="html"><![CDATA[Exploring the Innovations in Flownet paper approach for optical flow estimation.]]></summary></entry><entry><title type="html">ImageNet Classification with Deep Convolutional Neural Networks</title><link href="https://alphapibeta.github.io/blog/2024/imagenet/" rel="alternate" type="text/html" title="ImageNet Classification with Deep Convolutional Neural Networks"/><published>2024-01-18T00:00:00+00:00</published><updated>2024-01-18T00:00:00+00:00</updated><id>https://alphapibeta.github.io/blog/2024/imagenet</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2024/imagenet/"><![CDATA[<h2 id="1-introduction">1. Introduction</h2> <blockquote> <p><strong>Convolutional neural networks (CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies). Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse.</strong></p> </blockquote> <ol> <li><strong>Stationarity of Statistics</strong>: Images’ statistical properties, such as texture and color distributions, are assumed to be consistent across different regions of the image. This enables CNNs to apply the same filters to the entire image, significantly reducing the model’s complexity.</li> <li><strong>Locality of Pixel Dependencies</strong>: CNNs operate under the assumption that nearby pixels are more likely to be connected. In order to enable the network to recognize local features like edges and shapes, this is accomplished by applying tiny, localized filters (kernels) to the image.</li> </ol> <p>Mathematically, a convolution operation in a CNN can be represented as:</p> \[f(x, y) = \sum_{i=-a}^{a} \sum_{j=-b}^{b} K(i, j) \cdot I(x-i, y-j)\] <p>Here, ( f(x, y) ) is the output after applying the convolution operation to the input image ( I ) at position ( (x, y) ). The kernel ( K(i, j) ) is applied over the image such that each element of the kernel is multiplied by the corresponding element of the image under it, and the results are summed to produce the output ( f(x, y) ). The kernel ( K ) can be expressed as a matrix:</p> \[K = \begin{bmatrix} k_{11} &amp; k_{12} &amp; k_{13} \\ k_{21} &amp; k_{22} &amp; k_{23} \\ k_{31} &amp; k_{32} &amp; k_{33} \end{bmatrix}\] <p>With fewer parameters than fully connected networks, CNNs can learn hierarchical representations of images thanks to these principles, which makes them particularly useful for tasks like object detection and image classification.</p> <h2 id="2-dataset">2. Dataset</h2> <p>ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories.</p> <h2 id="3-the-architecture">3. The Architecture</h2> <blockquote> <p><strong>3.1 ReLU Nonlinearity</strong></p> <blockquote> <p><strong>In terms of training time with gradient descent, these saturating nonlinearities are much slower than the non-saturating nonlinearity f(x) = max(0,x). Following Nair and Hinton [20], we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs). Deep convolutional neural net- works with ReLUs train several times faster than their equivalents with tanh units.</strong></p> </blockquote> </blockquote> <p>The activation function is a core component in neural networks that introduces non-linearity, affecting the network’s ability to learn complex patterns. Traditional activation functions like the hyperbolic tangent (<code class="language-plaintext highlighter-rouge">tanh</code>) or the sigmoid function can cause the gradient to vanish during backpropagation, which slows down the training.</p> <p>The Rectified Linear Unit (ReLU) has become the de facto standard in neural network activation due to its non-saturating form, defined as:</p> \[ReLU(x) = \max(0, x)\] <p>This simple yet powerful function maintains the gradient, preventing the vanishing gradient problem and allowing the network to learn faster. Neurons with ReLU activation are only activated when the input is positive, leading to sparse activation within the network.</p> <p>CNNs with ReLUs train substantially faster than those with <code class="language-plaintext highlighter-rouge">tanh</code> units. This efficiency gain is depicted in the figure below, which demonstrates how a four-layer convolutional network with ReLUs can reach a 25% training error rate on CIFAR-10 much quicker than the same network utilizing <code class="language-plaintext highlighter-rouge">tanh</code> neurons.</p> <div class="row mt-10"> <div class="col-md-6 col-sm-2 mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/imagenet_relu-480.webp 480w,/assets/img/imagenet_relu-800.webp 800w,/assets/img/imagenet_relu-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/imagenet_relu.png" class="img-fluid rounded z-depth-4 img-smaller" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 1: A comparison of training times between a CNN with ReLU activation and one with tanh activation.</div> </div> </div> <p>Empirical results show that networks with ReLU not only train faster but also achieve better performance on complex tasks, which is particularly beneficial when dealing with large datasets and deep architectures.</p> <blockquote> <p><strong>3.3 Local Response Normalization</strong></p> <blockquote> <p><strong>ReLUs have the desirable property that they do not require input normalization to prevent them from saturating. If at least some training examples produce a positive input to a ReLU, learning will happen in that neuron. However, we still find that the following local normalization scheme aids generalization.</strong></p> </blockquote> </blockquote> <h3 id="the-role-in-relu-networks">The Role in ReLU Networks</h3> <p>While Rectified Linear Units (ReLUs) have the advantage of not requiring input normalization to prevent saturation, we discovered an additional normalization technique that enhances generalization. This is known as Local Response Normalization (LRN).</p> <h3 id="mechanism-of-lrn">Mechanism of LRN</h3> <p>Given the activity \(a*{i}^{x,y}\) of a neuron, calculated by applying \(kernel ( i )\) at position \((x, y)\) and then passing it through a ReLU nonlinearity, the response-normalized activity \(( b*{i}^{x,y} )\) can be described by:</p> \[b_{i}^{x,y} = \frac{a_{i}^{x,y}}{\left(k + \alpha \sum_{j=\max(0, i-n/2)}^{\min(N-1, i+n/2)} (a_{j}^{x,y})^2\right)^\beta}\] <p>Here, the sum runs over ( n ) “adjacent” kernel maps at the same spatial position, and ( N ) represents the total number of kernels in the layer. This normalization is inspired by lateral inhibition found in biological neurons, inducing competition among neuron outputs computed with different kernels.</p> <blockquote> <p><strong>3.4 Overlapping Pooling</strong></p> </blockquote> <p>If a pooling layer has a grid of pooling units, they are spaced <code class="language-plaintext highlighter-rouge">s</code> pixels apart and each summarizes a <code class="language-plaintext highlighter-rouge">z × z</code> neighborhood. Traditional pooling sets <code class="language-plaintext highlighter-rouge">s = z</code>, meaning no overlap between the neighborhoods.</p> <p><strong>However</strong>, Imagenet employs an overlapping pooling strategy. They set <code class="language-plaintext highlighter-rouge">s &lt; z</code>. Specifically, we use <code class="language-plaintext highlighter-rouge">s = 2</code> and <code class="language-plaintext highlighter-rouge">z = 3</code>. <strong>This overlapping approach has been found to be more effective in reducing error rates</strong>.</p> <blockquote> <p><strong>3.5 Overall Architecture</strong></p> <blockquote> <p><strong>As depicted in Figure 2, the net contains eight layers with weights; the first five are convolutional and the remaining three are fully- connected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution.</strong></p> </blockquote> </blockquote> <p>The approximately 60 million trainable parameters of Imagenet are a crucial component. This large number of parameters improves the network’s classification performance by enabling it to learn intricate and subtle features from the vast and diverse ImageNet dataset. Throughout the training phase, these parameters are changed to allow the network to perform as well as possible on the dataset.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/imagenet-480.webp 480w,/assets/img/imagenet-800.webp 800w,/assets/img/imagenet-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/imagenet.png" class="img-fluid rounded z-depth-5" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 2: Architecture of a Convolutional Neural Network.</div> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Imagenet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2)) // 23,232 parameters
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) // 307,200 parameters
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) // 663,552 parameters
    (7): ReLU(inplace=True)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) // 884,736 parameters
    (9): ReLU(inplace=True)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) // 589,824 parameters
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True) // 37,748,736 parameters
    (2): ReLU(inplace=True)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=4096, bias=True) // 16,777,216 parameters
    (5): ReLU(inplace=True)
    (6): Linear(in_features=4096, out_features=1000, bias=True) // 4,096,000 parameters
  )
)
</code></pre></div></div> <blockquote> <p><strong>4 Reducing Overfitting</strong></p> <p><strong>Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC make each training example impose 10 bits of constraint on the mapping from image to label, this turns out to be insufficient to learn so many parameters without considerable overfitting. Below, we describe the two primary ways in which we combat overfitting.</strong></p> </blockquote> <blockquote> <p><strong>4.1 Data Augmentation</strong></p> <blockquote> <p>We employ two distinct forms of data augmentation</p> </blockquote> </blockquote> <ul> <li>Generating image translations and horizontal reflections</li> <li>PCA Color Augmentation</li> </ul> <h2 id="pca-color-augmentation">PCA Color Augmentation</h2> <p>PCA Color Augmentation is a technique to augment image data by adjusting the intensities of the RGB channels. This method helps the model become more invariant to changes in illumination, which is a common property of natural images.</p> <p>The process involves the following steps:</p> <ol> <li>Perform PCA on the RGB pixel values across the entire ImageNet training set to obtain principal components.</li> <li>For each training image, alter the pixel values by adding a certain amount of these principal components.</li> </ol> <p>Mathematically, for each pixel \(I\_{xy} = [I_R, I_G, I_B]^T\) in the image, we add the following quantity:</p> \[[p_1, p_2, p_3][\alpha_1 \lambda_1, \alpha_2 \lambda_2, \alpha_3 \lambda_3]^T\] <p>Here:</p> <ul> <li>\(p_i\) represents the \(i\)-th eigenvector from the PCA of the RGB pixel values.</li> <li>\(\lambda_i\) is the corresponding eigenvalue, which indicates the variance in the direction of the eigenvector.</li> <li>\(\alpha_i\) is a random variable drawn from a Gaussian distribution with mean zero and standard deviation 0.1. It is sampled once per image and used across all pixels, ensuring consistency within the image.</li> </ul> <p>By scaling the principal components by their corresponding eigenvalues and the random variable, we effectively simulate realistic variations in lighting. This adjustment to the pixel values does not alter the content or structure of the image, preserving the label of the image while providing a form of regularization.</p> <blockquote> <p><strong>4.2 Dropout</strong></p> <blockquote> <p><strong>The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in back- propagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.</strong></p> </blockquote> </blockquote> <p>To stop overfitting in neural networks, dropout is a regularization technique used. The network is forced to learn robust features that are independent of any particular set of neurons by randomly setting a portion of the neuron outputs to zero during training. Overfitting is essentially reduced by the randomness in neuron activation, which improves the model’s ability to generalize to new data. Neuron outputs are adjusted to compensate for the deactivated neurons during training, but dropout is not applied during testing.</p> <h2 id="results">Results</h2> <h2 id="comparison-of-model-performances">Comparison of Model Performances</h2> <p>The paper presents a comparison of the convolutional neural network (CNN) model with other notable methods. Below is a summary of their performances on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) for the years 2010 and 2012:</p> <table> <thead> <tr> <th>Model</th> <th>Top-1 Error (ILSVRC-2010)</th> <th>Top-5 Error (ILSVRC-2010)</th> <th>Top-5 Error (ILSVRC-2012)</th> </tr> </thead> <tbody> <tr> <td>CNN</td> <td>37.5%</td> <td>17.0%</td> <td>15.3%</td> </tr> <tr> <td>Sparse Coding [2]</td> <td>47.1%</td> <td>28.2%</td> <td>26.2% (val)</td> </tr> <tr> <td>SIFT + FVs [24]</td> <td>45.7%</td> <td>25.7%</td> <td>-</td> </tr> </tbody> </table> <h3 id="analysis">Analysis</h3> <ul> <li><strong>CNN</strong>: Demonstrates a notable decrease in the top-1 and top-5 error rates. This demonstrates how deep convolutional neural networks work well for tasks involving image classification.</li> <li><strong>Sparse Coding [2]</strong>: Even though it works well, this approach has higher error rates than CNN, which highlights how much better deep learning techniques perform.</li> <li><strong>SIFT + FVs [24]</strong>: It shows higher error rates, just like Sparse Coding. The comparison demonstrates how CNNs have improved at handling challenging image recognition tasks.</li> </ul> <p>This comparative analysis illustrates the groundbreaking impact of CNNs in reducing error rates and setting new benchmarks in image classification.</p> <h2 id="conclusion">Conclusion</h2> <p>By utilizing the expansive ImageNet dataset, the network achieved remarkable accuracy, showcasing a significant leap over previous state-of-the-art results. Key innovations include the use of ReLU nonlinearity, dropout for overfitting reduction, and extensive data augmentation, contributing to the network’s robust performance.</p> <hr/> <h3 id="citation">Citation</h3> <p>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. NIPS.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="Deep-Learning,"/><category term="Computer-Vision,"/><category term="AIResearch"/><summary type="html"><![CDATA[Understanding Novalties of Imagenet classification]]></summary></entry><entry><title type="html">Efficient Estimation of Word Representations in Vector Space-2013</title><link href="https://alphapibeta.github.io/blog/2024/Word2Vec/" rel="alternate" type="text/html" title="Efficient Estimation of Word Representations in Vector Space-2013"/><published>2024-01-18T00:00:00+00:00</published><updated>2024-01-18T00:00:00+00:00</updated><id>https://alphapibeta.github.io/blog/2024/Word2Vec</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2024/Word2Vec/"><![CDATA[<h2 id="1-abstract">1. Abstract</h2> <blockquote> <p><strong>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previ- ously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfor- mance on our test set for measuring syntactic and semantic word similarities..</strong></p> </blockquote> <p>This paper introduces innovative model architectures that diverge from traditional N-gram models in statistical language modeling, offering a fresh approach to comprehend relationships within a vocabulary. These models prioritize computational efficiency while maintaining high accuracy in representing word semantics and syntactic relationships, showcasing a significant advancement in natural language processing.</p> <h2 id="11-goals-of-the-paper">1.1 Goals of the Paper</h2> <blockquote> <p><strong>The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary.We use recently proposed techniques for measuring the quality of the resulting vector representa- tions, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity.</strong></p> </blockquote> <h2 id="2-model-architectures">2. Model Architectures</h2> <blockquote> <p><strong>In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.</strong></p> </blockquote> <p>The training complexity for the models is given by:</p> \[O = E \times T \times Q\] <p>where ( O ) represents the overall training complexity, ( E ) is the number of training epochs, ( T ) denotes the number of words in the training set, and ( Q ) is a model-specific factor.</p> <p>Models are trained using stochastic gradient descent and backpropagation.</p> <p><br/></p> <blockquote> <p><strong>2.1 Feedforward Neural Net Language Model (NNLM)</strong></p> <blockquote> <p>The Feedforward Neural Network Language Model (NNLM) is structured with several layers: input, projection, hidden, and output.</p> </blockquote> </blockquote> <h3 id="input-layer">Input Layer</h3> <p>At the input layer, the model considers ( N ) previous words. These words are encoded using 1-of-( V ) coding, where ( V ) represents the size of the vocabulary.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
</code></pre></div></div> <h3 id="projection-layer">Projection Layer</h3> <p>The encoded input is then mapped to a projection layer ( P ) with dimensionality ( N \times D ), using a shared projection matrix. This step is computationally efficient since only ( N ) inputs are active simultaneously.</p> <h3 id="hidden-layer">Hidden Layer</h3> <p>The architecture’s complexity increases between the projection and hidden layers due to the dense nature of projection layer values. For a typical choice of ( N = 10 ), the projection layer size (( P )) ranges from 500 to 2000, while the hidden layer (( H )) usually comprises 500 to 1000 units.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">context_size</span> <span class="o">*</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
</code></pre></div></div> <h3 id="output-layer">Output Layer</h3> <p>The hidden layer is essential for computing the probability distribution over the vocabulary words, resulting in an output layer with dimensionality ( V ). The computational complexity per training example can be expressed as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
</code></pre></div></div> \[Q = N \times D + N \times D \times H + H \times V\] <p>In this equation, ( H \times V ) is the dominant term. However, to mitigate this complexity, several practical solutions have been proposed. These include using hierarchical versions of the softmax or employing models that</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">100</span>   
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">128</span>      
<span class="n">context_size</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># NNLM Model Structure
</span><span class="nc">NNLM</span><span class="p">(</span>
  <span class="p">(</span><span class="n">embeddings</span><span class="p">):</span> <span class="nc">Embedding</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="o">//</span> <span class="mi">100</span><span class="p">,</span><span class="mi">000</span> <span class="n">parameters</span>
  <span class="p">(</span><span class="n">linear1</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">//</span> <span class="mi">38</span><span class="p">,</span><span class="mi">528</span> <span class="n">parameters</span>
  <span class="p">(</span><span class="n">linear2</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">//</span> <span class="mi">129</span><span class="p">,</span><span class="mi">000</span> <span class="n">parameters</span>
<span class="p">)</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">1000000</span>  
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">100</span>   
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">128</span>      
<span class="n">context_size</span> <span class="o">=</span> <span class="mi">3</span>      

<span class="c1"># NNLM Model Structure with Larger Vocabulary
</span><span class="nc">NNLM</span><span class="p">(</span>
  <span class="p">(</span><span class="n">embeddings</span><span class="p">):</span> <span class="nc">Embedding</span><span class="p">(</span><span class="mi">1000000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="o">//</span> <span class="mi">100</span><span class="p">,</span><span class="mi">000</span><span class="p">,</span><span class="mi">000</span> <span class="n">parameters</span>
  <span class="p">(</span><span class="n">linear1</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">//</span> <span class="mi">38</span><span class="p">,</span><span class="mi">528</span> <span class="n">parameters</span>
  <span class="p">(</span><span class="n">linear2</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">//</span> <span class="mi">128</span><span class="p">,</span><span class="mi">999</span><span class="p">,</span><span class="mi">872</span> <span class="n">parameters</span>
<span class="p">)</span>
</code></pre></div></div> <h3 id="hierarchical-softmax">Hierarchical Softmax</h3> <p>The model utilize hierarchical softmax, where the vocabulary is represented as a Huffman binary tree. This approach capitalizes on the observation that word frequency effectively determines classes in neural net language models. Huffman trees assign shorter binary codes to more frequent words, reducing the number of output units needing evaluation. While a balanced binary tree would require evaluating \(( \log_2(V) )\) outputs, a Huffman tree-based hierarchical softmax needs only about \(( \log_2( ext{Unigram perplexity}(V)) )\). This results in a significant speedup, especially beneficial when the vocabulary size is large, such as one million words.</p> <blockquote> <p><strong>2.2 Recurrent Neural Net Language Model (RNNLM)</strong></p> <blockquote> <p><strong>Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N ), and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and output layer. What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections. This allows the recurrent model to form some kind of short term memory, as information from the past can be represented by the hidden layer state that gets updated based on the current input and the state of the hidden layer in the previous time step.</strong></p> </blockquote> </blockquote> <p>The complexity per training example of the RNN model is given by:</p> \[Q = H \times H + H \times V\] <p>where the word representations \(( D )\) have the same dimensionality as the hidden layer \(( H )\). Again, the term \(( H \times V )\) can be efficiently reduced to \(( H \times \log_2(V) )\) by using hierarchical softmax. Most of the complexity then comes from \(( H \times H )\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">RNNLM</span><span class="p">(</span>
  <span class="p">(</span><span class="n">rnn</span><span class="p">):</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span> <span class="o">//</span> <span class="mi">1296640</span> <span class="n">parameters</span>
  <span class="p">(</span><span class="n">linear</span><span class="p">):</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">//</span> <span class="mi">1290000</span> <span class="n">parameters</span>
<span class="p">)</span>
</code></pre></div></div> <h2 id="3-new-log-linear-models">3. New Log-linear Models</h2> <blockquote> <p><strong>In this section, we propose two new model architectures for learning distributed representations of words that try to minimize computational complexity. The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model. While this is what makes neural networks so attractive, we decided to explore simpler models that might not be able to represent the data as precisely as neural networks, but can possibly be trained on much more data efficiently.</strong></p> <blockquote> <p><strong>3.1 Continuous Bag-of-Words Model</strong></p> <blockquote> <p><strong>The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged). We call this archi- tecture a bag-of-words model as the order of words in the history does not influence the projection. Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word.</strong></p> </blockquote> </blockquote> </blockquote> <p>The training complexity of the CBOW model is captured by the formula:</p> \[Q = N \times D + D \times \log_2(V)\] <p>Here, ( Q ) is the training complexity, ( N ) is the number of context words (both past and future), ( D ) is the size of the word embeddings, and ( V ) is the vocabulary size. This model is denoted as CBOW, distinguishing it from traditional bag-of-words models by its use of continuous distributed representations for context.</p> <h5 id="note-that-the-weight-matrix-between-the-input-and-the-projection-layer-is-shared-for-all-word-positions-in-the-same-way-as-in-the-nnlm"><strong>Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.</strong></h5> <blockquote> <blockquote> <p><strong>3.2 Continuous Skip-gram Model</strong></p> <blockquote> <p><strong>The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. More precisely, we use each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word. We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples.</strong></p> </blockquote> </blockquote> </blockquote> <p>The training complexity for the Skip-gram model is given by:</p> \[Q = C \times (D + D \times \log_2(V))\] <p>Here, ( Q ) denotes the training complexity, ( C ) represents the context window size, ( D ) is the dimensionality of the word embeddings, and ( V ) is the vocabulary size. The model effectively performs classifications for each word, using both preceding and following words within the context window as positive examples, with distant words weighted less significantly.</p> <div class="row mt-10"> <div class="col-md-12 col-sm-12 mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/word2vec-480.webp 480w,/assets/img/word2vec-800.webp 800w,/assets/img/word2vec-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/word2vec.png" class="img-fluid rounded z-depth-4" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption">Figure 1: New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.</div> </div> </div> <h2 id="4-results-and-observations">4. Results and Observations</h2> <blockquote> <p><strong>We found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented..</strong></p> </blockquote> <p>Through algebraic operations on word vectors, we can uncover relationships and analogies, bringing a new dimension to understanding language semantics. The method is simple: subtract the vector representing one concept from another, then add a third, and search for the nearest word vector to this result. For instance, to find a word related to ‘small’ in the same way ‘biggest’ is related to ‘big’, we compute:</p> \[\text{vector(smallest)} \approx \text{vector(biggest)} - \text{vector(big)} + \text{vector(small)}\] <p>This computation, when applied to well-trained word vectors, often yields the correct association.</p> <h2 id="5-comparative-analysis-of-model-architectures">5. Comparative Analysis of Model Architectures</h2> <blockquote> <p><strong>A comparative study of various word vector models shows that simpler architectures like CBOW and Skip-gram can surpass more complex neural networks in both syntactic and semantic tasks, underscoring the potential of log-linear models in processing vast datasets.</strong></p> </blockquote> <p>Comparative performance of word vector models on semantic and syntactic accuracy:</p> <table> <thead> <tr> <th>Model Architecture</th> <th>Semantic Accuracy (%)</th> <th>Syntactic Accuracy (%)</th> </tr> </thead> <tbody> <tr> <td>RNNLM</td> <td>9</td> <td>36</td> </tr> <tr> <td>NNLM</td> <td>23</td> <td>53</td> </tr> <tr> <td>CBOW</td> <td>24</td> <td>64</td> </tr> <tr> <td>Skip-gram</td> <td>55</td> <td>59</td> </tr> </tbody> </table> <p><br/></p> <h5 id="the-skip-gram-model-in-particular-shines-in-its-ability-to-understand-semantic-relationships-outperforming-other-models-this-is-a-testament-to-the-potential-of-these-simpler-models-in-handling-large-scale-data-effectively">The Skip-gram model, in particular, shines in its ability to understand semantic relationships, outperforming other models. This is a testament to the potential of these simpler models in handling large-scale data effectively</h5> <p><br/></p> <h2 id="conclusion">Conclusion</h2> <blockquote> <p><strong>In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set..</strong></p> </blockquote> <h3 id="citation">Citation</h3> <p>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. arXiv preprint arXiv:1301.3781.</p> <p><em>Date: January 21, 2024</em></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="Deep-Learning,"/><category term="NLP,"/><category term="AIResearch"/><summary type="html"><![CDATA[Understanding Novalties of Word Representations in Vector Space (Word2Vec).]]></summary></entry><entry><title type="html">Introduction to Kernel density estimation.</title><link href="https://alphapibeta.github.io/blog/2019/introduction-to-kernel-density-estimation/" rel="alternate" type="text/html" title="Introduction to Kernel density estimation."/><published>2019-08-31T07:37:27+00:00</published><updated>2019-08-31T07:37:27+00:00</updated><id>https://alphapibeta.github.io/blog/2019/introduction-to-kernel-density-estimation</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2019/introduction-to-kernel-density-estimation/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Box-Cox Transformation.</title><link href="https://alphapibeta.github.io/blog/2019/box-cox-transformation/" rel="alternate" type="text/html" title="Box-Cox Transformation."/><published>2019-08-25T11:04:02+00:00</published><updated>2019-08-25T11:04:02+00:00</updated><id>https://alphapibeta.github.io/blog/2019/box-cox-transformation</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2019/box-cox-transformation/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Autoencoders — Escape the curse of dimensionality.</title><link href="https://alphapibeta.github.io/blog/2019/autoencodersescape-the-curse-of-dimensionality/" rel="alternate" type="text/html" title="Autoencoders — Escape the curse of dimensionality."/><published>2019-04-01T07:36:30+00:00</published><updated>2019-04-01T07:36:30+00:00</updated><id>https://alphapibeta.github.io/blog/2019/autoencodersescape-the-curse-of-dimensionality</id><content type="html" xml:base="https://alphapibeta.github.io/blog/2019/autoencodersescape-the-curse-of-dimensionality/"><![CDATA[]]></content><author><name></name></author></entry></feed>